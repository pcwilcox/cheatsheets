% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.5pc,itemsep=0.25em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother

\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}

\begin{document}
    \footnotesize
    \begin{multicols}{4}
    \setlength{\premulticols}{1pt}
    \setlength{\postmulticols}{1pt}
    \setlength{\multicolsep}{1pt}
    \setlength{\columnsep}{2pt}

    \subparagraph*{Permutations: } 
        Given an array of \(n\) elements the first position can be filled with \(n\) different elements, the second with \(n-1\), and so on. \(n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 1 = n!\)

        \begin{itemize}
            \item[] \(P_{n,k} = \frac{n!}{(n-k)!}\)
            \item[] \(P_{n,n} = n!\)
        \end{itemize}

    \subparagraph*{Combinations: }
        In general we can ``combine'' \(n\) elements taking \(k\) at a time in \(C_{n,k} = \frac{P_{n,k}}{k!} = \frac{n!}{(n-k)!k!} = {n \choose k} \). 

    \subparagraph*{Multinomial Coefficients: } 
        Consider splitting \(n\) elements into \(k (k \geq 2)\) groups in a way such that group \(j\) gets \(n_j\) elements and \(\sum_{j = 1}^{k}n_j = n\). The \(n_1\) elements in the first group can be selected in \({n \choose n_1}\), the second in \({n-n_1 \choose n_2}\), the third in \({n-n_1-n_2 \choose n_3}\) and so on until we complete the \(k\) groups. Then: \({n \choose n_1}\cdot{n-n_1 \choose n_2}\cdot{n-n_1-n_2 \choose n_3}\cdot \ldots \cdot{n_k \choose n_k} = {n \choose n_1, n_2, \ldots, n_k}\)

    \subparagraph*{Probability of union:}
        If \(A_1, A_2, \ldots, A_n\) are \textit{disjoint events} then 
        \begin{itemize}
            \item[] \(\Pr(A_1 \cup A_2 \cup \ldots \cup A_n) = \Pr(\bigcup\limits_{i=1}^{n}A_i) \)
            \item[] \(= \Pr(A_1) + \Pr(A_2) + \ldots + \Pr(A_n) \)
            \item[] \(= \sum\limits_{i=1}^{n}\Pr(A_i) \)
        \end{itemize}
        \smallskip

    If the events are not disjoint: 
        \begin{itemize}
            \item[] Two events \(A_1, A_2: \Pr(A_1 \cup A_2) = \Pr(A_1) + \Pr(A_2) - \Pr(A_1 \cap A_2)\)
            \item[] Three events: \(A_1, A_2, A_3: \Pr(A_1 \cup A_2 \cup A_3)\)
            \item[] \(= \Pr(A_1) + \Pr(A_2) + \Pr(A_3) - \Pr(A_1 \cap A_2) - \Pr(A_1 \cap A_3) - \Pr(A_2 \cap A_3) + \Pr(A_1 \cap A_2 \cap A_3)\)
        \end{itemize}  

    \subparagraph*{Conditional Probability: }
        If \(A, B\) are events such that \(\Pr(A) > 0\) and \(\Pr(B) > 0\) then 
        \begin{itemize}
            \item[] \(\Pr(B|A) = \frac{\Pr(A\cap B)}{\Pr(A)}\) and 
            \item[] \(\Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)}\) 
            \item[] Furthermore: 
            \item[] \(\Pr(A \cap B) = \Pr(B|A) \cdot \Pr(A)\) and 
            \item[] \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B)\) 
            \item[] In general: \(\Pr(A_1 \cap A_2 \cap \ldots \cap A_n) \)
            \item[] \(= \Pr(A_1) \cdot \Pr(A_2 | A_1) \cdot \ldots \cdot \Pr(A_n | A_1 \cap A_2 \cap \ldots \cap A_{n-1})\)
        \end{itemize}

    \subparagraph*{Independence:}
        \(A, B\) are independent events if \(\Pr(A|B) = \Pr(A)\) and \(\Pr(B|A) = \Pr(B)\). Then, if \(A, B\) are independent: 
        \begin{itemize}
            \item[] \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B) = \Pr(A) \cdot \Pr(B)\) and 
            \item[] \(\Pr(A \cap B) = \Pr(B | A) \cdot \Pr(A) = \Pr(B) \cdot \Pr(A)\).
            \item[] In general if \(A_1, A_2, \ldots, A_n\) are independent: 
            \item[] \(\Pr(A_1 \cap A_2 \cap \ldots \cap A_n) = \Pr(A_1) \cdot \Pr(A_2) \cdot \ldots \cdot \Pr(A_n)\). 
            \item[] Note that if \(A \cap B = \emptyset\) then the two events are \textit{not independent}. 
            \item[] Note that if \(A, B\) are independent then \(A, B^c\) are also independent.
        \end{itemize}

    \subparagraph*{Conditionally Independent: } 
        \(A_1, A_2, \ldots, A_k\) are \textit{conditionally independent} given \(B\) if, for every subset \(A_{i_1}, \ldots, A_{i_m}: \Pr(A_{i_1} \cap \ldots \cap A_{i_m} | B) = \Pr(A_{i_1} | B) \cdot \ldots \cdot \Pr(A_{i_m} | B)\).

    \subparagraph*{Bayes' Theorem:}
        Let \(B_1, \ldots, B_k :=\) a partition of \(S\) such that \(\Pr(B_j) > 0, j \in 1, \ldots, k\). Assume you have \(A\) such that \(\Pr(A) > 0\). Then: \(\Pr(B_i|A) = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\Pr(A)} = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\sum\limits_{j=1}^{k}\Pr(A|B_j) \cdot \Pr(B_j)}\)

    \subparagraph*{Uniform Distribution: }
        \(X = x, x \in \{1, 2, \ldots, k\}\) with all values \(x\) equally likely. The p.f. is \(f_X(x) =\\ \Pr(X=x) =  \begin{cases} \frac      {1}{k} & x = 1, 2, \ldots, k \\ 0 & o.w. \end{cases} \)

    \subparagraph*{Bernoulli Distribution: }
        An event \(A\) happens with probability \(p\):
        \begin{itemize}
            \item[] \(X = \begin{cases} 1 & \text{if \(A\) happens} \\ 0 & \text{if \(A^c\) happens} \end{cases} \) 
            \item[] \(f_X(x) =
    \begin{cases} 
    (1-p)   & x = 0 \\ 
    p       & x = 1 \\
    0       & \text{o.w.} 
    \end{cases} \)
\end{itemize}

    \subparagraph*{Binomial:}
        \(n\) Bernoulli trials repeated independently with probability of success \(p\). 
        \begin{itemize}
            \item[] \(X :=\) number of success in \(n\) trials. 
            \item[] \(x \in \{0, 1, \ldots, n\}\). 
            \item[] \(f_X(x) = \Pr(X=x)\)
            \item[] \( =
    \begin{cases}
    {n \choose x} p^x (1-p)^{n-x}   &   x=0, \ldots, n \\
    0                               &   \text{o.w.}
    \end{cases} \)
            \item[] \(\rightarrow X \sim Bin(n,p)\)
        \end{itemize}

        \subparagraph*{Quantile Function: }
        \(X\) continuous r.v. \(F^{-1}(p)\) is the quantile function of \(X\) for \(0 \leq p \leq 1. F^{-1}(p) = x \Rightarrow p = F(x)\).

    \subparagraph*{Joint Continuous Distributions: }
    Joint p.d.f. given by \(f_{X,Y}(x,y) = \Pr((X,Y) \in A) = \iint\limits_{A} f(x,y)dxdy\). To find the joint c.d.f. just integrate.

        \subparagraph*{Negative-Binomial: }
        \begin{itemize}
            \item[] We repeat Bernoulli trials until \(r\) successes are observed. 
            \item[] \(X :=\) number of failures \(= \{0, 1, \ldots\}\). 
            \item[] \(p :=\) probability of success. 
            \item[] \(\Pr(X = x) = \Pr(x \text{ failures before \(r\) successes}) \)
            \item[] \( = \Pr(x\) failures and \(r-1\) successes in \(x + r - 1\) trials\()) \cdot \Pr(\)one success in last trial\()\)
            \item[] \(= \left[{x+r-1 \choose x } (1-p)^x p^{r-1} \right] \cdot p = {x + r - 1 \choose x} (1 - p)^x p^r\).
            \item[] \(f_X(x) =
    \begin{cases}
    {x+r-1 \choose x} (1-p)^x p^r   &   x=0,1,2,\ldots \\
    0       & \text{o.w.}
    \end{cases}\)
\end{itemize}
    \subparagraph*{Hypergeometric: }
        \begin{itemize}
            \item[] A box with \(A\) red balls and \(B\) blue balls.
            \item[] \(n\) balls are drawn \textit{without replacement}. 
            \item[] \(X :=\) number of red balls. 
            \item[] \(X \leq \min(n, A)\). 
            \item[] \(\max(n-B, 0) \leq X \leq \min(n,A) \rightarrow f_X(x) = \)
            \item[] \(
    \begin{cases}
    \frac{ {A \choose x} \cdot {B \choose n - x}}{{A + B \choose n}}     &    \max(n-B,0) \leq x \leq \min(n,A) \\
    0   &   \text{o.w.}
    \end{cases}
    \)
\end{itemize}

    

    \subparagraph*{Geometric: }
        Negative binomial with \(r=1\)
        \begin{itemize}
            \item[] \(f_X(x) = \begin{cases} {(1-p)}^x p & x=0,1,\ldots \\ 0 & \text{o.w.} \end{cases}\)
        \end{itemize}

    \subparagraph*{Poisson: }
        Counts occurences of an event. \(X\) is a Poisson r.v. with parameter \(\lambda\) (intensity) if the p.f. is \(f_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots \\ 0 & \text{o.w.} \end{cases}\) with \(\lambda>0\).

    \subparagraph*{Continuous Random Variables: }
        A r.v. \(X\) has a cont. distribution if there is a non-negative \(f\) s.t. \(\Pr(a \leq X \leq b) = \int_{a}^{b} f(x)dx\). \(f\) is the probability density function p.d.f. To find the normalizing constant integrate the p.d.f. over the domain, it must equal 1.

    \subparagraph*{Cumulative Distribution Function: } 
        (c.d.f.) For any r.v. \(X\) the c.d.f. is given by \(F(x) = \Pr(X \leq x)\). Properties: 
        \begin{itemize}
            \item[] \(\forall x: 0 \leq F(x) \leq 1\)
            \item[] \(F(x)\) is non-decreasing, i.e. if \(x_1 < x_2 \Rightarrow \{X \leq x_1 \} \subset \{X \leq x_2 \}\) and so \(\Pr(X \leq x_1) \leq \Pr(X \leq x_2) \Rightarrow F(x_1) \leq F(x_2)\)
            \item[] \(\lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty} F(x) = 1\)
        \end{itemize}
        \smallskip
        For a continuous r.v.:
        \begin{itemize}
            \item[] \(F(x) = \Pr(X \leq x) = \int_{-\infty}^{\infty}f(t)dt\)
            \item[] \(F'(x) = f(x)\)
            \item[] \(\Pr(a < X \leq b) = \Pr(a \leq X \leq b) = \Pr(a \leq X < b) = \Pr(a < X < b)\)
        \end{itemize}
        \smallskip
    In general, \(X \sim Unif[a,b] \Rightarrow f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text{o.w.} \end{cases}\). \\
    The c.d.f.: \(F(x) = \begin{cases} 0 & x < a \\ \frac{x-a}{b-a} & a \leq x \leq b \\ 1 x > b \end{cases}\)



    \subparagraph*{Marginal Distributions: }
        In general for discrete r.v. \(f_X(x) = \sum_{y}f(x,y)\) and \(f_Y(y) = \sum_{x}f(x,y)\). In the case of 2 cont. r.v. \(f_X(x) = \int_{-\infty}^{\infty}f(x,y)dy\) and \(f_Y(y) = \int_{-\infty}^{\infty}f(x,y)dx\). \(f_X(x)\) is the marginal p.d.f. of \(X\). \(f_Y(y)\) is the marginal p.d.f. of \(Y\).

    \subparagraph*{Independence: }
        Two r.v. are independent if they produce independent events: \(\Pr(X \in A, Y \in B) = \Pr(X \in A) \cdot \Pr(Y \in B)\). This implies: \(\Pr(X \leq x, Y \leq y) = \Pr(X \leq x) \cdot \Pr(Y \leq y) \Rightarrow F(x,y) = F_X(x) \cdot F_Y(y)\).

    \subparagraph*{Conditional Distributions: }
        \(X, Y\) discrete r.v. with p.f. \(f_X(x), f_Y(y)\) and joint p.f. \(f(x,y)\). Then:
        \begin{itemize}
            \item[] \(\Pr(X = x | Y = y) = \frac{\Pr(X = x, Y = y)}{\Pr(Y = y)} = \frac{f(x,y)}{f_Y(y)}\).
        \end{itemize}
        This is a new distribution and the p.f. is (p.f. of \((X|Y)\)):
        \begin{itemize}
            \item[] \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & \forall x,y: f_Y(y) > 0 \\ 0 & \text{o.w.}\end{cases}\)
        \end{itemize}
        Note that 
        \begin{itemize}
            \item[] \(\sum_x{y_X(x|y)} = \sum_x{\frac{f(x,y)}{f_Y(y)}} \)
             \(= \frac{1}{f_Y(y)} \cdot \sum_x{f(x,y)} = \frac{f_Y(y)}{f_Y(y)} = 1\). 
            \item[]
        \end{itemize}
    We can also define the conditional distribution of \(Y\) given \(X=x\) by:
        \begin{itemize}
            \item[] \(g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & \forall x,y: f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}\)
        \end{itemize}
    In the continuous case \(X, Y\) with joint p.d.f. \(f(x,y)\) and marginal p.d.f.'s \(f_X(x)\) and \(f_Y(y)\): 
\begin{itemize}
\item[] \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & f_Y(y) > 0 \\ 0 & \text{o.w.} \end{cases} \)
\item[] \(g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}\).
\end{itemize}
 Again note that 
 \begin{itemize}
    \item[] \(\int\limits_{-\infty}^{\infty}g_X(x|y)dx = \int\limits_{-\infty}^{\infty} \frac{f(x,y)}{f_Y(y)}dx = \frac{1}{f_Y(y)} \int\limits_{-\infty}^{\infty}f(x,y)dx \)
    \item[] \(= \frac{1}{f_Y(y)} \cdot f_Y(y) = 1\).
 \end{itemize}

\subparagraph*{Multivariate Distributions:}
 \(X_1, X_2, \ldots, X_n\) have a joint discrete distribution if \((X_, \dots, X_n)\) can have only a countable sequence of values in \(\mathcal{R}^n\). The joint p.f. is \(f(x_1,\dots,x_n) = \Pr(X_1=x_1, X_2=x_2,\ldots,X_n=x_n)\). \(X_1, X_2, \ldots, X_n\) have a joint continuous distribution if there exists \(f\) such that \(f\left((X_1,\ldots,X_n)\in \mathcal{C}\right) = \int\limits_{\mathcal{C}}\dots\int f(x_1,\ldots,x_n)dx_1\ldots dx_n\). Here \(f(x_1,\ldots,x_n)\) is the joint p.d.f. and \(f(x_1,\ldots,x_n) = \frac{d^nF(x_1,\ldots,x_n)}{dx_1\ldots dx_2}\). \(F(x_1,\ldots,x_n) = \Pr(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n)\).

 \subparagraph*{Marginal Distributions: }
    \(X_1, \ldots, X_n\) are cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Then the marginal distribution of \(X_1 = f_{X_1}(x_1) = \int\limits_{- \infty}^{\infty} \dots \int\limits_{- \infty}^{\infty} f(x_1,x_2, \ldots, x_n)dx_2dx_3\ldots dx_n\). \(F(x_1)\) is the marginal c.d.f. of \(X_1\) and \(F(x_1) = \Pr(X_1 \leq x_1) = \Pr(X_1 \leq x_1, X_2 < \infty, \ldots, X_n < \infty)\).

\subparagraph*{Conditional Distribution: }
 \(f(x_1, \ldots, x_n)\): joint p.d.f. of \(x_1, \ldots, x_n\); \(f_0(x_1, \ldots, x_k)\): joint p.d.f. of \(x_1, \ldots, x_k\) with \(k < n\). Then \(\forall x_1,\ldots, x_k\) such that \(f_0(x_1, \ldots, x_k) > 0\) the conditional p.d.f. of \(X_{k+1},\ldots,X_n\) given \(X_1 = x_1, \ldots, X_k = x_k\) is \(g(x_{k+1},\ldots,x_n | x_1, \ldots, x_k) = \frac{f(x_1, \ldots, x_n)}{f_0(x_1, \ldots, x_k)}\).

 \subparagraph*{Functions of one R.V: }
    Consider a r.v. \(X\) cont. with p.d.f. \(f_X(x)\). Assume we are interested in \(Y=r(X)\) with \(r\) a function. What is the dist. of \(Y\)? Let \(G_Y(y) = \Pr(Y \leq y)\) be the c.d.f. of \(Y\): \(G_Y(y) = \Pr(Y \leq y) = \Pr(r(X) \leq y) = \int{f(x)}dx \{x: r(x) \leq y\} \). To get the p.d.f. of \(Y\) take derivatives: \(g_Y(y) = \frac{dG_Y(y)}{dy} = \frac{1}{2} y^{-\frac{1}{2}}\). For continuous r.v. such that \(Y = r(X)\) with \(r\) differentiable and one-to-one: \(g_Y(y) = f_X(r^{-1}(y))\left|\frac{d}{dy} r^{-1} (y) \right|\).

\subparagraph*{Functions of two or more R.V.s:}
 Discrete case: \(X_1, X_2, \ldots, X_n\) r.v. with joint p.f. \(f(x_1, \ldots, x_n\):
\(Y_1 = r_1(X_1,\ldots,X_n)\) \dots \(Y_m = r_m(X_1,\ldots,X_n)\). Let \(A := \{(x_1,\ldots,x_n)\):= such that \(y_1 = r(x_1,\ldots,x_n) \dots y_m = r(x_1,\ldots,x_n)\)\}. Then: \(g(y_1,\ldots,y_m)=\Pr(Y_1=y_1,\ldots,Y_m=y_m)=\sum\limits_{(x_1,\ldots,x_n)\in A}{f(x_1,\ldots,x_n)}\) \\

Continuous case: \(X_1, X_2, \ldots, X_n\) cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Let \(Y = r(X_1,\ldots,X_n) \rightarrow A_y = \{(x_1,\ldots,x_n)\) s.t. \(r(x_1,\ldots,x_n) \leq y\}\) Then the c.d.f. of \(Y\) is \(G(y) = \Pr(Y \leq y) = \Pr\left( r(X_1,\ldots,X_n) \leq y \right) = \int\limits_{A_y}\cdots\int{f(x_1,\ldots,x_n)}dx_1dx_2\ldots dx_n\). The density/p.d.f. of \(Y\) is \(g(y) = \frac{dG(y)}{dy}\).

\subparagraph*{Transformations: }
 \(X_1,\ldots,X_n\) cont. r.v.'s with joint pdf \(f(x_1,\ldots,x_n)\). Let \(Y_1 = r_1(X_1,\ldots,X_n), \dots, Y_n = r_n (X_1, \ldots, X_n)\). To find the joint pdf of \(Y_1, \ldots, Y_2\) for a one-to-one differentiable transformation \(x_1 = s_1 (y_1,\ldots,y_n), \dots, x_n = s_n (y_1, \ldots, y_n) \rightarrow \) The joint pdf of \(Y_1,\ldots,Y_n\) is \(g(y_1,\ldots,y_n) = f(s_1,\ldots,s_n)\left|J\right|\) where \(J = \det \begin{bmatrix}
    \frac{ds_1}{dy_1} & \dots & \frac{ds_1}{dy_n} \\
    \vdots & \ddots & \vdots \\
     \frac{ds_n}{dy_1} & \hdots & \frac{ds_n}{dy_n}
 \end{bmatrix}
 \)

 \subparagraph*{Linear Transformations: } Suppose that 
 \(\vec{X} = 
    \begin{pmatrix}
        X_1\\
        \vdots{}\\ 
        X_n
    \end{pmatrix}\) and 
\(\vec{Y} = 
    \begin{pmatrix}
        Y_1\\
        \vdots{} \\
        Y_n
    \end{pmatrix}
    = A\vec{X}\) (with \(A\) a non-singular matrix). Then \(\vec{X} = A^{-1}\vec{Y}\) and \(g_Y(y) = f_X(A^{-1})\cdot\frac{1}{\left|\det A\right|}\)

\subparagraph*{Markov Chains: }
A sequence of r.v.'s \(X_1, X_2, \ldots\) is a stochastic process with discrete time parameter. \(X_1\) is the initial state and \(X_n\) is the state at time \(n\). A stochastic process with discrete time parameter is a Markov chain if for each \(n\), \(\Pr(X_{n+1} \leq b | X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = \Pr(X_{n+1} = \leq b | X_n = x_n)\). A Markov Chain is finite if there are finite possible states. Then: \(\Pr(X_1 = x_1, \ldots, X_n = x_n) = \Pr(X_1 = x_1) \Pr(X_2 = x_2 | X_1 = x_1) \cdots \Pr(X_n = x_n | X_{n-1} = x_{n-1}) \). Transition distributions: When a MC has \(k\) possible states then it has a transition distribution where there exist probabilities \(p_{ij}\) for \(i, j = 1, \ldots , k\) such that \(\forall n: \Pr(X_{n+1} = j | X_n = i) = p_{ij}\) and if \(\Pr(X_{n+1} = j | X_n = i) = p_{ij} \forall n\) then it is a stationary transition distribution. In this case there is a matrix s.t. \(\sum\limits_{j=1}^{k}{p_{ij}}=1,  \forall i\): \\
\(P = \begin{bmatrix} 
        p_{11} & \hdots & p_{1k} \\
        \vdots & \ddots & \vdots \\
         p_{k1} & \hdots & p_{kk}
     \end{bmatrix}\)\\     
Transition of several steps: \(P^m = P \cdot \ldots \cdot P\) Just exponentiate \(P\) and then find the resulting \(p_{ij}\).

\subparagraph*{Expectation: }
\begin{itemize}
    \item[] \( E(X) = \int\limits_{-\infty}^{\infty}{x f(x)dx}\)
    \item[] \( E(X) = \sum\limits_{x}xf(x) = \sum\limits_{x}\Pr(X=x)\)
    \item[] If \(Y = r(X)\) and \(f(x)\) is the p.d.f. of \(X: E(Y) = \int\limits_{-\infty}^{\infty}{r(x)f(x)dx}\)
    \item[] \(Y = aX + b \rightarrow E(Y) = aE(X)+b\)
    \item[] \(a\) constant s.t. \(\Pr(X \geq a) = 1\) then \(E(X) \geq a\)
    \item[] \(b\) constant s.t. \(\Pr(X \leq b) = 1\) then \(E(X) \leq b\).
    \item[] If \(X_1, \ldots, X_n \) are r.v. then \(E(X_1+\ldots+X_n) = E(X_1) + \ldots +E(X_n)\)
    \item[] \(E\left( \sum_{i = 1}^{n}X_i \right) = \sum_{i = 1}^{n}(E(X_i))\)
    \item[] \(X_1, \ldots, X_n\) independent r.v.'s with finite expectation: \(E\left(\Pi_{i=1}^{n}{X_i} \right) = \Pi_{i=1}^{n}\left(E(X_i)\right)\)
    \item[] Bernoulli(\(p\)): \(E(X) = p\)
    \item[] Binomial(\(n,p\)): \(E(X) = np\)
    \item[] Poisson: \(E(X) = \lambda\)
    \item[] Geometric: \(E(X) = \frac{1-p}{p}\)
    \item[] Negative Binomial: \(E(X) = \frac{r(1-p)}{p}\)
\end{itemize}

\subparagraph*{Variance: }
\begin{itemize}
    \item[] \(V(X) = E[(X-\mu)^x]\) with \(\mu = E(X)\)
    \item[] S.D.: \(\sigma = \sqrt{V(X)}\)
    \item[] \(V(X) \geq 0\)!!!
    \item[] \(X\) discrete: \(V(X) = \sum\limits_X{(x-\mu)^2f(x)}\)
    \item[] \(X\) cont.: \(V(X) = \int\limits_{-\infty}^{\infty}{(x-\mu)^2f(x)dx}\)
    \item[] \(V(X) = E[(X-\mu)^2] = E(X^2) - \mu^2\)
    \item[] \(V(X) = 0 \iff \Pr(X = c) = 1\) with \(c\) constant
    \item[] \(a, b\) constant: \(V(aX+b) = a^2V(X)\)
    \item[] \(X_1,\ldots,X_n\) independent: \(V(X_1+\ldots+X_n) = V(X_1)+ \ldots + V(X_n)\)
    \item[] Bernoulli: \(V(X) = p(1-p)\)
    \item[] Binomial: \(V(X) = np(1-p)\)
    \item[] Poisson: \(V(X) = \lambda\)
    \item[] Geometric: \(V(X) = \frac{1-p}{p^2}\)
    \item[] Negative Binomial: \(V(X) = \frac{r(1-p)}{p^2}\)
\end{itemize}

\subparagraph*{Covariance: }
\begin{itemize}
    \item[] Cov\((X,Y) = E[(X-\mu_X)(Y-\mu_Y)] = E(XY) - \mu_X\mu_Y\)
    \item[] Discrete: \(E(XY) = \sum\limits_x\sum\limits_yxyf(x,y)\)
    \item[] Discrete: \(\mu_X = E(X) = \sum\limits_x\sum\limits_yxf(x,y)\)
    \item[] Discrete: \(\mu_Y = E(Y) = \sum\limits_x\sum\limits_yyf(x,y)\)
    \item[] Cont: \(E(XY) = \int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}{xyf(x,y)dxdy}\)
    \item[] If \(X\) and \(Y\) are independent: Cov\((X,Y) = 0\)
\end{itemize}
\subparagraph*{Correlation: }
\begin{itemize}
    \item[] Corr\((X,Y) = \rho(X,Y)\)
    \item[] \(= \frac{\text{Cov}(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}} =\frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}\)
    \item[] Schwarz Ineq: \([E(UV)]^2 \leq E(U^2)E(V^2)\)
    \item[] \([\text{Cov}(X,Y)]^2 \leq V(X) \cdot V(Y)\)
    \item[] \( -1 \leq \rho(X,Y) \leq 1\)
    \item[] Indep: Cov\((X,Y) = 0\) and \(\rho(X,Y) = 0\)
    \item[] \(X\) r.v. w/ finite variance and \(Y = aX+b\) s.t. \(a \neq 0, a,b,\) constant, then
    \item[] \(a > 0 \rightarrow \rho(X,Y) = 1\) and
    \item[] \(a < 0 \rightarrow \rho(X,Y) = -1\)
    \item[] \(X,Y\) w/ finite var. then \(V(X+Y) = V(X)+V(Y) + 2\cdot \text{Cov}(X,Y)\)
    \item[] \(V(aX + bY) = a^2 V(X) + b^2 V(Y) + 2ab\cdot\text{Cov}(X,Y)\)
\end{itemize}


    \subparagraph*{Other Stuff:}
    \begin{itemize}
        \item[] \(\sum\limits_{i=1}^{n}i = \frac{n(n+1)}{2},\sum\limits_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}\)
        \item[] \(\sum\limits_{i=0}^{n}c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1; \sum\limits_{i=0}^{\infty}c^i = \frac{1}{1-c}\)
        \item[] \(\frac{d}{dx}(a^x) = \ln a\)
        \item[] \((fg)' = f'g + fg'\)
        \item[] \(\left(\frac{f}{g}\right)' = \frac{f'g-fg'}{g^2}\)
        \item[] \(\frac{d}{dx}(f(g(x)))=f'(g(x))g'(x)\)
        \item[] \(\frac{d}{dx}(e^{g(x)})= g'(x)e^{g(x)}\)
        \item[] \(\frac{d}{dx}(\ln g(x)) = \frac{g'(x)}{g(x)}\)
        \item[] \(\int \frac{1}{x}dx = \ln |x| + c\), \(\int \frac{1}{ax+b}dx=\frac{1}{a}\ln |ax+b|+c\)
        \item[] \(\int e^u du = e^u + c\), \(\int {a^u}du = \frac{a^u}{\ln a} + c\)
        \item[] \(\int_{a}^{b}{f(g(x))g'(x)}dx \Rightarrow u=g(x) \Rightarrow \int_{g(a)}^{g(b)}f(u)du\)
        \item[] \(\int udv = uv - \int vdu\)
    \end{itemize}
    \end{multicols}

\end{document}