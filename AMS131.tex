% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.5pc,itemsep=0.25em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother

\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}

\begin{document}
    \footnotesize
    \begin{multicols}{4}
    \setlength{\premulticols}{1pt}
    \setlength{\postmulticols}{1pt}
    \setlength{\multicolsep}{1pt}
    \setlength{\columnsep}{2pt}

    \subparagraph*{Fundamentals: } 
        \begin{itemize}[noitemsep]
            \item[] \(x \in S: A \cup B= \{x \in A \text{ or } x \in B\}\)
            \item[] \(A \cup B = B \cup A\), \(A \cup A = A\)
            \item[] \(A \cup \emptyset = A\), \(A \cup S = S \)
            \item[] \(A \subset B \Rightarrow A \cup B = B\)
            \item[] \(A_1, A_2, \dots, A_n \Rightarrow A_1 \cup A_2 \cup \dots \cup A_n = \bigcup\limits_{i=1}^{i=n} A_i\)
            \item[] \(\bigcup\limits_{i=1}^{\infty} A_i \rightarrow \bigcup\limits_{i\in I}A_i\)
            \item[] \((A \cup B) \cup C = A \cup (B \cup C) = A \cup B \cup C \)
            \item[] \( A \cap B = \{ x \in A \text{ and } x \in B\} = AB \)
            \item[] \(A \cap B = B \cap A \), \(A \cap A = A \)
            \item[] \(A \cap \emptyset = \emptyset \), \(A \cap S = A\)
            \item[] \(A \subset B \Rightarrow A \cap B\)
            \item[] \(\bigcap\limits_{i\in I}A_i = \bigcap\limits_{i=1}^{\infty}A_i\)
            \item[] \(\bigcap\limits_{i=1}^{n}A_i = A_1 \cap A_2 \cap \dots \cap A_n \)
            \item[] \( (A \cap B) \cap C = A \cap (B \cap C) = A \cap B \cap C\)
            \item[] \(A^c = \{x \in S: x \notin A\}\)
            \item[] \((A^c)^c = A\), \(\emptyset^c = S\), \(S^c = \emptyset\)
            \item[] \(A\cup A^c = S\), \(A \cap A^c = \emptyset\)
        \end{itemize}

    \subparagraph*{Disjoint Events: } 
        \(A\) and \(B\) are \textit{disjoint} or \textit{mutually exclusive} if \(A\) and \(B\) have no outcomes in common. This happens only if \(A \cap B = \emptyset \). A collection \(A_1, \dots, A_n\) is a collection of disjoint evens if and only if \(A_i \cap A_j = \emptyset, \forall i, j, i \neq j\)

        \begin{itemize}[noitemsep]
            \item[] \(\left (\bigcup\limits_{i\in I} A_i \right)^c = \bigcap\limits_{i\in I}A_i^c \)
            \item[] \((A\cup B)^c = A^c \cap B^c \)
            \item[] \(x\in(A\cap B)^c 
            \Rightarrow x \notin A\) and \(x\notin B \)
            \item[] \(\Rightarrow x \in A^c \) and \(x \in B^c \)
            \item[] \(\Rightarrow x \in A^c \cap B^c \)
        \end{itemize}

    \subparagraph*{Probabilities: } 

        \begin{itemize}[noitemsep]
            \item[] \(\forall A: \Pr(A) \geq 0\)
            \item[] \(\Pr(S) = 1\), \(\Pr(\emptyset) = 0\)
            \item[] \(\Pr(A^c) = 1-\Pr(A)\)
            \item[] \(A \subset B \Rightarrow \Pr(A) \leq \Pr(B)\)
            \item[] \(\forall A: 0 \leq \Pr(A) \leq 1\)
        \end{itemize}
        For every \textit{infinite sequence} of \textit{disjoint} events: \( A_1, A_2, \dots (A_i \in S)\):
        \begin{itemize}
            \item[] \(\Pr\left(\bigcup\limits_{i=1}^{\infty}A_i\right) = \sum\limits_{i=1}^{\infty} \Pr(A_i)\)
            \item[] \(=  \Pr(A_1 \cup A_2 \cup \dots \cup A_n \cup \dots)\)
            \item[] \(= \Pr(A_1) + \Pr(A_2) + \dots + \Pr(A_n) + \dots\)
            \item[] \(\Pr(\bigcup\limits_{i=1}^{n}A_i) = \Pr(\bigcup\limits_{i=1}^{n}A_i + \bigcup\limits_{i=n+1}^{\infty}\emptyset) \)
            \item[] \(= \sum\limits_{i=1}^{n}\Pr(A_i)\)
            \item[] \(\Pr(A\cup B) = \Pr(A) + \Pr(B) - \Pr(A\cap B)\)
        \end{itemize}

    \subparagraph*{Finite Sample Spaces: } 
        \(S := \{s_1, s_2, \dots, s_n \}\), \(\Pr(s_i) = P_i, \forall i = 1, 2, \dots, n\), s.t. \(\sum\limits_{i=1}^{n}P_i = 1\). A sample space \(S\) with \(n\) outcomes \(s_1, \ldots, s_n\) is a \textit{simple sample space} if the probability assigned to each outcome is \(\frac{1}{n}\). If \(A\) contains \(m\) outcomes then \(\Pr(A) = \frac{m}{n}\).

    \subparagraph*{Multiplication Rule: } 
        An experiment has \(k\) parts \((k \geq 2)\) s.t. the \(i^{th}\) part has \(n_i\) possible outcomes, \(i = 1, \ldots, k\), and \textit{all possible outcomes can occur regardless of which outcomes have occurred in other parts}. \(S\) will contain vectors of the form \((u_1, u_2, \ldots, u_k)\). \(u_i\) is one of the \(n_i\) possible outcomes of part \(i\). The total number of vectors is \(n_1 \cdot n_2 \cdot \ldots \cdot n_k\).

    \subparagraph*{Permutations: } 
        Given an array of \(n\) elements the first position can be filled with \(n\) different elements, the second with \(n-1\), and so on. \(n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 1 = n!\)

        \begin{itemize}
            \item[] \(P_{n,k} = \frac{n!}{(n-k)!}\)
            \item[] \(P_{n,n} = n!\)
        \end{itemize}

    \subparagraph*{Combinations: }
        In general we can ``combine'' \(n\) elements taking \(k\) at a time in \(C_{n,k} = \frac{P_{n,k}}{k!} = \frac{n!}{(n-k)!k!} = {n \choose k} \). 

    \subparagraph*{Multinomial Coefficients: } 
        Consider splitting \(n\) elements into \(k (k \geq 2)\) groups in a way such that group \(j\) gets \(n_j\) elements and \(\sum_{j = 1}^{k}n_j = n\). The \(n_1\) elements in the first group can be selected in \({n \choose n_1}\), the second in \({n-n_1 \choose n_2}\), the third in \({n-n_1-n_2 \choose n_3}\) and so on until we complete the \(k\) groups. Then: \({n \choose n_1}\cdot{n-n_1 \choose n_2}\cdot{n-n_1-n_2 \choose n_3}\cdot \ldots \cdot{n_k \choose n_k} = {n \choose n_1, n_2, \ldots, n_k}\)

    \subparagraph*{Probability of union:}
        If \(A_1, A_2, \ldots, A_n\) are \textit{disjoint events} then 
        \begin{itemize}
            \item[] \(\Pr(A_1 \cup A_2 \cup \ldots \cup A_n) = \Pr(\bigcup\limits_{i=1}^{n}A_i) \)
            \item[] \(= \Pr(A_1) + \Pr(A_2) + \ldots + \Pr(A_n) \)
            \item[] \(= \sum\limits_{i=1}^{n}\Pr(A_i) \)
        \end{itemize}
        \smallskip

    If the events are not disjoint: 
        \begin{itemize}
            \item[] Two events \(A_1, A_2: \Pr(A_1 \cup A_2) = \Pr(A_1) + \Pr(A_2) - \Pr(A_1 \cap A_2)\)
            \item[] Three events: \(A_1, A_2, A_3: \Pr(A_1 \cup A_2 \cup A_3)\)
            \item[] \(= \Pr(A_1) + \Pr(A_2) + \Pr(A_3) - \Pr(A_1 \cap A_2) - \Pr(A_1 \cap A_3) - \Pr(A_2 \cap A_3) + \Pr(A_1 \cap A_2 \cap A_3)\)
        \end{itemize}  

    \subparagraph*{Conditional Probability: }
        If \(A, B\) are events such that \(\Pr(A) > 0\) and \(\Pr(B) > 0\) then 
        \begin{itemize}
            \item[] \(\Pr(B|A) = \frac{\Pr(A\cap B)}{\Pr(A)}\) and 
            \item[] \(\Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)}\) 
            \item[] Furthermore: 
            \item[] \(\Pr(A \cap B) = \Pr(B|A) \cdot \Pr(A)\) and 
            \item[] \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B)\) 
            \item[] In general: \(\Pr(A_1 \cap A_2 \cap \ldots \cap A_n) \)
            \item[] \(= \Pr(A_1) \cdot \Pr(A_2 | A_1) \cdot \ldots \cdot \Pr(A_n | A_1 \cap A_2 \cap \ldots \cap A_{n-1})\)
        \end{itemize}

    \subparagraph*{Independence:}
        \(A, B\) are independent events if \(\Pr(A|B) = \Pr(A)\) and \(\Pr(B|A) = \Pr(B)\). Then, if \(A, B\) are independent: 
        \begin{itemize}
            \item[] \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B) = \Pr(A) \cdot \Pr(B)\) and 
            \item[] \(\Pr(A \cap B) = \Pr(B | A) \cdot \Pr(A) = \Pr(B) \cdot \Pr(A)\).
            \item[] In general if \(A_1, A_2, \ldots, A_n\) are independent: 
            \item[] \(\Pr(A_1 \cap A_2 \cap \ldots \cap A_n) = \Pr(A_1) \cdot \Pr(A_2) \cdot \ldots \cdot \Pr(A_n)\). 
            \item[] Note that if \(A \cap B = \emptyset\) then the two events are \textit{not independent}. 
            \item[] Note that if \(A, B\) are independent then \(A, B^c\) are also independent.
        \end{itemize}

    \subparagraph*{Conditionally Independent: } 
        \(A_1, A_2, \ldots, A_k\) are \textit{conditionally independent} given \(B\) if, for every subset \(A_{i_1}, \ldots, A_{i_m}: \Pr(A_{i_1} \cap \ldots \cap A_{i_m} | B) = \Pr(A_{i_1} | B) \cdot \ldots \cdot \Pr(A_{i_m} | B)\).

    \subparagraph*{Partitions: }
        Let \(B_1, \ldots, B_k\) be such that \(B_i \cap B_j = \emptyset \forall i \neq j\) and \(\bigcup_{i = 1}^{k} B_i = S\). Then these events form a partition of \(S\).
        \begin{itemize}
            \item[] \(A = A \cap S = A \cap \left(\bigcup_{i = 1}^{k} B_i\right) \)
            \item[] \(= (A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_k)\). Then: 
            \item[] \(\Pr(A) = \Pr(A \cap S) \)
            \item[] \(= \Pr(A \cap \left(\bigcup_{i = 1}^{k} B_i \right)) \)
            \item[] \(= \Pr(A \cap B_1) + \Pr(A \cap B_2) + \ldots + \Pr(A \cap B_k)\)
            \item[] \(=\Pr(A|B_1) \cdot \Pr(B_1) + \Pr(A|B_2)\cdot \Pr(B_2) + \ldots + \Pr(A|B_k) \cdot \Pr(B_k) \)
            \item[] \(= \sum_{i=1}^{k} \Pr(A|B_i)\cdot \Pr(B_i)\). 
            \item[]So, if \(B_1, \ldots, B_k\) are a partition of \(S\): \(\Pr(A) = \sum_{i=1}^{k}\Pr(A|B_i)\cdot \Pr(B_i)\)
        \end{itemize}

    \subparagraph*{Bayes' Theorem:}
        Let \(B_1, \ldots, B_k :=\) a partition of \(S\) such that \(\Pr(B_j) > 0, j \in 1, \ldots, k\). Assume you have \(A\) such that \(\Pr(A) > 0\). Then: \(\Pr(B_i|A) = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\Pr(A)} = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\sum\limits_{j=1}^{k}\Pr(A|B_j) \cdot \Pr(B_j)}\)

    \subparagraph*{Random Variables:}
        A real-valued function on \(S\) is a  random variable. A random variable \(X\) is a functions that assigns a real number \(X(s)=x\) to each possible outcome \(s \in S\): \(X:S \Rightarrow \mathcal{D}\). \(x:=\) a realization of the random variable, \(x \in \mathcal{D}\). We will be computing \(\Pr(X \in E)\) for \(E \subset \mathcal{D} = \Pr(s \in S: X(s) \in E)\)
    
    \subparagraph*{Discrete Probability Distributions: }
    A r.v. \(X\) has a discrete distribution if it takes a countable number of values. The probability function of a discrete r.v. is \(f_X(x) = \Pr(X=x)\). Properties:
    \begin{itemize}
        \item[] \(0 \leq f_X(x) \leq 1\)
        \item[] \(\forall x \notin \mathcal{D}: f_X(x) = 0\)
        \item[] \(\sum\limits_{x \in \mathcal{D}} f_X(x) = 1\)
        \item[] \(\Pr(X \in A) = \sum\limits_{x \in A} f_X (x)\)
    \end{itemize}

    \subparagraph*{Uniform Distribution: }
        \(X = x, x \in \{1, 2, \ldots, k\}\) with all values \(x\) equally likely. The p.f. is \(f_X(x) =\\ \Pr(X=x) =  \begin{cases} \frac      {1}{k} & x = 1, 2, \ldots, k \\ 0 & o.w. \end{cases} \)

    \subparagraph*{Bernoulli Distribution: }
        An event \(A\) happens with probability \(p\):
        \begin{itemize}
            \item[] \(X = \begin{cases} 1 & \text{if \(A\) happens} \\ 0 & \text{if \(A^c\) happens} \end{cases} \) 
            \item[] \(f_X(x) =
    \begin{cases} 
    (1-p)   & x = 0 \\ 
    p       & x = 1 \\
    0       & \text{o.w.} 
    \end{cases} \)
\end{itemize}

    \subparagraph*{Binomial:}
        \(n\) Bernoulli trials repeated independently with probability of success \(p\). 
        \begin{itemize}
            \item[] \(X :=\) number of success in \(n\) trials. 
            \item[] \(x \in \{0, 1, \ldots, n\}\). 
            \item[] \(f_X(x) = \Pr(X=x)\)
            \item[] \( =
    \begin{cases}
    {n \choose x} p^x (1-p)^{n-x}   &   x=0, \ldots, n \\
    0                               &   \text{o.w.}
    \end{cases} \)
            \item[] \(\rightarrow X \sim Bin(n,p)\)
        \end{itemize}

        \subparagraph*{Quantile Function: }
        \(X\) continuous r.v. \(F^{-1}(p)\) is the quantile function of \(X\) for \(0 \leq p \leq 1. F^{-1}(p) = x \Rightarrow p = F(x)\).

    \subparagraph*{Joint Continuous Distributions: }
    Joint p.d.f. given by \(f_{X,Y}(x,y) = \Pr((X,Y) \in A) = \iint\limits_{A} f(x,y)dxdy\). To find the joint c.d.f. just integrate.

        \subparagraph*{Negative-Binomial: }
        \begin{itemize}
            \item[] We repeat Bernoulli trials until \(r\) successes are observed. 
            \item[] \(X :=\) number of failures \(= \{0, 1, \ldots\}\). 
            \item[] \(p :=\) probability of success. 
            \item[] \(\Pr(X = x) = \Pr(x \text{ failures before \(r\) successes}) \)
            \item[] \( = \Pr(x\) failures and \(r-1\) successes in \(x + r - 1\) trials\()) \cdot \Pr(\)one success in last trial\()\)
            \item[] \(= \left[{x+r-1 \choose x } (1-p)^x p^{r-1} \right] \cdot p = {x + r - 1 \choose x} (1 - p)^x p^r\).
            \item[] \(f_X(x) =
    \begin{cases}
    {x+r-1 \choose x} (1-p)^x p^r   &   x=0,1,2,\ldots \\
    0       & \text{o.w.}
    \end{cases}\)
\end{itemize}
    \subparagraph*{Hypergeometric: }
        \begin{itemize}
            \item[] A box with \(A\) red balls and \(B\) blue balls.
            \item[] \(n\) balls are drawn \textit{without replacement}. 
            \item[] \(X :=\) number of red balls. 
            \item[] \(X \leq \min(n, A)\). 
            \item[] \(\max(n-B, 0) \leq X \leq \min(n,A) \rightarrow f_X(x) = \)
            \item[] \(
    \begin{cases}
    \frac{ {A \choose x} \cdot {B \choose n - x}}{{A + B \choose n}}     &    \max(n-B,0) \leq x \leq \min(n,A) \\
    0   &   \text{o.w.}
    \end{cases}
    \)
\end{itemize}

    

    \subparagraph*{Geometric: }
        Negative binomial with \(r=1\)
        \begin{itemize}
            \item[] \(f_X(x) = \begin{cases} {(1-p)}^x p & x=0,1,\ldots \\ 0 & \text{o.w.} \end{cases}\)
        \end{itemize}

    \subparagraph*{Poisson: }
        Counts occurences of an event. \(X\) is a Poisson r.v. with parameter \(\lambda\) (intensity) if the p.f. is \(f_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots \\ 0 & \text{o.w.} \end{cases}\) with \(\lambda>0\).

    \subparagraph*{Continuous Random Variables: }
        A r.v. \(X\) has a cont. distribution if there is a non-negative \(f\) s.t. \(\Pr(a \leq X \leq b) = \int_{a}^{b} f(x)dx\). \(f\) is the probability density function p.d.f. To find the normalizing constant integrate the p.d.f. over the domain, it must equal 1.

    \subparagraph*{Cumulative Distribution Function: } 
        (c.d.f.) For any r.v. \(X\) the c.d.f. is given by \(F(x) = \Pr(X \leq x)\). Properties: 
        \begin{itemize}
            \item[] \(\forall x: 0 \leq F(x) \leq 1\)
            \item[] \(F(x)\) is non-decreasing, i.e. if \(x_1 < x_2 \Rightarrow \{X \leq x_1 \} \subset \{X \leq x_2 \}\) and so \(\Pr(X \leq x_1) \leq \Pr(X \leq x_2) \Rightarrow F(x_1) \leq F(x_2)\)
            \item[] \(\lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty} F(x) = 1\)
        \end{itemize}
        \smallskip
        For a continuous r.v.:
        \begin{itemize}
            \item[] \(F(x) = \Pr(X \leq x) = \int_{-\infty}^{\infty}f(t)dt\)
            \item[] \(F'(x) = f(x)\)
            \item[] \(\Pr(a < X \leq b) = \Pr(a \leq X \leq b) = \Pr(a \leq X < b) = \Pr(a < X < b)\)
        \end{itemize}
        \smallskip
    In general, \(X \sim Unif[a,b] \Rightarrow f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text{o.w.} \end{cases}\). \\
    The c.d.f.: \(F(x) = \begin{cases} 0 & x < a \\ \frac{x-a}{b-a} & a \leq x \leq b \\ 1 x > b \end{cases}\)



    \subparagraph*{Marginal Distributions: }
        In general for discrete r.v. \(f_X(x) = \sum_{y}f(x,y)\) and \(f_Y(y) = \sum_{x}f(x,y)\). In the case of 2 cont. r.v. \(f_X(x) = \int_{-\infty}^{\infty}f(x,y)dy\) and \(f_Y(y) = \int_{-\infty}^{\infty}f(x,y)dx\). \(f_X(x)\) is the marginal p.d.f. of \(X\). \(f_Y(y)\) is the marginal p.d.f. of \(Y\).

    \subparagraph*{Independence: }
        Two r.v. are independent if they produce independent events: \(\Pr(X \in A, Y \in B) = \Pr(X \in A) \cdot \Pr(Y \in B)\). This implies: \(\Pr(X \leq x, Y \leq y) = \Pr(X \leq x) \cdot \Pr(Y \leq y) \Rightarrow F(x,y) = F_X(x) \cdot F_Y(y)\).

    \subparagraph*{Conditional Distributions: }
        \(X, Y\) discrete r.v. with p.f. \(f_X(x), f_Y(y)\) and joint p.f. \(f(x,y)\). Then:
        \begin{itemize}
            \item[] \(\Pr(X = x | Y = y) = \frac{\Pr(X = x, Y = y)}{\Pr(Y = y)} = \frac{f(x,y)}{f_Y(y)}\).
        \end{itemize}
        This is a new distribution and the p.f. is (p.f. of \((X|Y)\)):
        \begin{itemize}
            \item[] \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & \forall x,y: f_Y(y) > 0 \\ 0 & \text{o.w.}\end{cases}\)
        \end{itemize}
        Note that 
        \begin{itemize}
            \item[] \(\sum_x{y_X(x|y)} = \sum_x{\frac{f(x,y)}{f_Y(y)}} \)
             \(= \frac{1}{f_Y(y)} \cdot \sum_x{f(x,y)} = \frac{f_Y(y)}{f_Y(y)} = 1\). 
            \item[]
        \end{itemize}
    We can also define the conditional distribution of \(Y\) given \(X=x\) by:
        \begin{itemize}
            \item[] \(g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & \forall x,y: f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}\)
        \end{itemize}
    In the continuous case \(X, Y\) with joint p.d.f. \(f(x,y)\) and marginal p.d.f.'s \(f_X(x)\) and \(f_Y(y)\): 
\begin{itemize}
\item[] \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & f_Y(y) > 0 \\ 0 & \text{o.w.} \end{cases} \)
\item[] \(g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}\).
\end{itemize}
 Again note that 
 \begin{itemize}
    \item[] \(\int\limits_{-\infty}^{\infty}g_X(x|y)dx = \int\limits_{-\infty}^{\infty} \frac{f(x,y)}{f_Y(y)}dx = \frac{1}{f_Y(y)} \int\limits_{-\infty}^{\infty}f(x,y)dx \)
    \item[] \(= \frac{1}{f_Y(y)} \cdot f_Y(y) = 1\).
 \end{itemize}

\subparagraph*{Multivariate Distributions:}
 \(X_1, X_2, \ldots, X_n\) have a joint discrete distribution if \((X_, \dots, X_n)\) can have only a countable sequence of values in \(\mathcal{R}^n\). The joint p.f. is \(f(x_1,\dots,x_n) = \Pr(X_1=x_1, X_2=x_2,\ldots,X_n=x_n)\). \(X_1, X_2, \ldots, X_n\) have a joint continuous distribution if there exists \(f\) such that \(f\left((X_1,\ldots,X_n)\in \mathcal{C}\right) = \int\limits_{\mathcal{C}}\dots\int f(x_1,\ldots,x_n)dx_1\ldots dx_n\). Here \(f(x_1,\ldots,x_n)\) is the joint p.d.f. and \(f(x_1,\ldots,x_n) = \frac{d^nF(x_1,\ldots,x_n)}{dx_1\ldots dx_2}\). \(F(x_1,\ldots,x_n) = \Pr(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n)\).

 \subparagraph*{Marginal Distributions: }
    \(X_1, \ldots, X_n\) are cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Then the marginal distribution of \(X_1 = f_{X_1}(x_1) = \int\limits_{- \infty}^{\infty} \dots \int\limits_{- \infty}^{\infty} f(x_1,x_2, \ldots, x_n)dx_2dx_3\ldots dx_n\). \(F(x_1)\) is the marginal c.d.f. of \(X_1\) and \(F(x_1) = \Pr(X_1 \leq x_1) = \Pr(X_1 \leq x_1, X_2 < \infty, \ldots, X_n < \infty)\).

\subparagraph*{Conditional Distribution: }
 \(f(x_1, \ldots, x_n)\): joint p.d.f. of \(x_1, \ldots, x_n\); \(f_0(x_1, \ldots, x_k)\): joint p.d.f. of \(x_1, \ldots, x_k\) with \(k < n\). Then \(\forall x_1,\ldots, x_k\) such that \(f_0(x_1, \ldots, x_k) > 0\) the conditional p.d.f. of \(X_{k+1},\ldots,X_n\) given \(X_1 = x_1, \ldots, X_k = x_k\) is \(g(x_{k+1},\ldots,x_n | x_1, \ldots, x_k) = \frac{f(x_1, \ldots, x_n)}{f_0(x_1, \ldots, x_k)}\).

 \subparagraph*{Functions of one R.V: }
    Consider a r.v. \(X\) cont. with p.d.f. \(f_X(x)\). Assume we are interested in \(Y=r(X)\) with \(r\) a function. What is the dist. of \(Y\)? Let \(G_Y(y) = \Pr(Y \leq y)\) be the c.d.f. of \(Y\): \(G_Y(y) = \Pr(Y \leq y) = \Pr(r(X) \leq y) = \int{f(x)}dx \{x: r(x) \leq y\} \). To get the p.d.f. of \(Y\) take derivatives: \(g_Y(y) = \frac{dG_Y(y)}{dy} = \frac{1}{2} y^{-\frac{1}{2}}\). For continuous r.v. such that \(Y = r(X)\) with \(r\) differentiable and one-to-one: \(g_Y(y) = f_X(r^{-1}(y))\left|\frac{d}{dy} r^{-1} (y) \right|\).

\subparagraph*{Functions of two or more R.V.s:}
 Discrete case: \(X_1, X_2, \ldots, X_n\) r.v. with joint p.f. \(f(x_1, \ldots, x_n\):
\(Y_1 = r_1(X_1,\ldots,X_n)\) \dots \(Y_m = r_m(X_1,\ldots,X_n)\). Let \(A := \{(x_1,\ldots,x_n)\):= such that \(y_1 = r(x_1,\ldots,x_n) \dots y_m = r(x_1,\ldots,x_n)\)\}. Then: \(g(y_1,\ldots,y_m)=\Pr(Y_1=y_1,\ldots,Y_m=y_m)=\sum\limits_{(x_1,\ldots,x_n)\in A}{f(x_1,\ldots,x_n)}\) \\

Continuous case: \(X_1, X_2, \ldots, X_n\) cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Let \(Y = r(X_1,\ldots,X_n) \rightarrow A_y = \{(x_1,\ldots,x_n)\) s.t. \(r(x_1,\ldots,x_n) \leq y\}\) Then the c.d.f. of \(Y\) is \(G(y) = \Pr(Y \leq y) = \Pr\left( r(X_1,\ldots,X_n) \leq y \right) = \int\limits_{A_y}\cdots\int{f(x_1,\ldots,x_n)}dx_1dx_2\ldots dx_n\). The density/p.d.f. of \(Y\) is \(g(y) = \frac{dG(y)}{dy}\).

\subparagraph*{Transformations: }
 \(X_1,\ldots,X_n\) cont. r.v.'s with joint pdf \(f(x_1,\ldots,x_n)\). Let \(Y_1 = r_1(X_1,\ldots,X_n), \dots, Y_n = r_n (X_1, \ldots, X_n)\). To find the joint pdf of \(Y_1, \ldots, Y_2\) for a one-to-one differentiable transformation \(x_1 = s_1 (y_1,\ldots,y_n), \dots, x_n = s_n (y_1, \ldots, y_n) \rightarrow \) The joint pdf of \(Y_1,\ldots,Y_n\) is \(g(y_1,\ldots,y_n) = f(s_1,\ldots,s_n)\left|J\right|\) where \(J = \det \begin{bmatrix}
    \frac{ds_1}{dy_1} & \dots & \frac{ds_1}{dy_n} \\
    \vdots & \ddots & \vdots \\
     \frac{ds_n}{dy_1} & \hdots & \frac{ds_n}{dy_n}
 \end{bmatrix}
 \)

 \subparagraph*{Linear Transformations: } Suppose that 
 \(\vec{X} = 
    \begin{pmatrix}
        X_1\\
        \vdots{}\\ 
        X_n
    \end{pmatrix}\) and 
\(\vec{Y} = 
    \begin{pmatrix}
        Y_1\\
        \vdots{} \\
        Y_n
    \end{pmatrix}
    = A\vec{X}\) (with \(A\) a non-singular matrix). Then \(\vec{X} = A^{-1}\vec{Y}\) and \(g_Y(y) = f_X(A^{-1})\cdot\frac{1}{\left|\det A\right|}\)



    \subparagraph*{Other Stuff:}
    \begin{itemize}
        \item[] \(\sum\limits_{i=1}^{n}i = \frac{n(n+1)}{2},\sum\limits_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}\)
        \item[] \(\sum\limits_{i=0}^{n}c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1; \sum\limits_{i=0}^{\infty}c^i = \frac{1}{1-c}\)
        \item[] \(\frac{d}{dx}(a^x) = \ln a\)
        \item[] \((fg)' = f'g + fg'\)
        \item[] \(\left(\frac{f}{g}\right)' = \frac{f'g-fg'}{g^2}\)
        \item[] \(\frac{d}{dx}(f(g(x)))=f'(g(x))g'(x)\)
        \item[] \(\frac{d}{dx}(e^{g(x)})= g'(x)e^{g(x)}\)
        \item[] \(\frac{d}{dx}(\ln g(x)) = \frac{g'(x)}{g(x)}\)
        \item[] \(\int \frac{1}{x}dx = \ln |x| + c\), \(\int \frac{1}{ax+b}dx=\frac{1}{a}\ln |ax+b|+c\)
        \item[] \(\int e^u du = e^u + c\), \(\int {a^u}du = \frac{a^u}{\ln a} + c\)
        \item[] \(\int_{a}^{b}{f(g(x))g'(x)}dx \Rightarrow u=g(x) \Rightarrow \int_{g(a)}^{g(b)}f(u)du\)
        \item[] \(\int udv = uv - \int vdu\)
    \end{itemize}
    \end{multicols}

\end{document}