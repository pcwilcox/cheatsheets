% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=0.5in]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.5pc,itemsep=0.25em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother

\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}

\begin{document}
    \footnotesize
    \begin{multicols}{4}
    \setlength{\premulticols}{1pt}
    \setlength{\postmulticols}{1pt}
    \setlength{\multicolsep}{1pt}
    \setlength{\columnsep}{2pt}

    \subparagraph*{Probability of union:}
        If \(A_1, A_2, \ldots, A_n\) are \textit{disjoint events} then \(\Pr(A_1 \cup A_2 \cup \ldots \cup A_n) = \Pr(\bigcup\limits_{i=1}^{n}A_i) = \Pr(A_1) + \Pr(A_2) + \ldots + \Pr(A_n) = \sum\limits_{i=1}^{n}\Pr(A_i) \)
        \smallskip
    If the events are not disjoint: \\
     Two events \(A_1, A_2: \Pr(A_1 \cup A_2) = \Pr(A_1) + \Pr(A_2) - \Pr(A_1 \cap A_2)\)\\
    Three events: \(A_1, A_2, A_3: \Pr(A_1 \cup A_2 \cup A_3)\)
    \(= \Pr(A_1) + \Pr(A_2) + \Pr(A_3) - \Pr(A_1 \cap A_2) - \Pr(A_1 \cap A_3) - \Pr(A_2 \cap A_3) + \Pr(A_1 \cap A_2 \cap A_3)\)
    
    \subparagraph*{Conditional Probability: }
    \(\Pr(B|A) = \frac{\Pr(A\cap B)}{\Pr(A)}\) and \(\Pr(A|B) = \frac{\Pr(A \cap B)}{\Pr(B)}\) \\
    \(\Pr(A \cap B) = \Pr(B|A) \cdot \Pr(A)\) and  \\
    \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B)\)  \\
    In general: \(\Pr(A_1 \cap A_2 \cap \ldots \cap A_n) \)\\
    \(= \Pr(A_1) \cdot \Pr(A_2 | A_1) \cdot \ldots \cdot \Pr(A_n | A_1 \cap A_2 \cap \ldots \cap A_{n-1})\)
    
    \subparagraph*{Independence:}
        \(A, B\) are independent events if \(\Pr(A|B) = \Pr(A)\) and \(\Pr(B|A) = \Pr(B)\). Then: \\
        \(\Pr(A \cap B) = \Pr(A|B) \cdot \Pr(B) = \Pr(A) \cdot \Pr(B)\) \\
        \(\Pr(A \cap B) = \Pr(B | A) \cdot \Pr(A) = \Pr(B) \cdot \Pr(A)\)\\
        In general if \(A_1, A_2, \ldots, A_n\) are independent: \(\Pr(A_1 \cap \ldots \cap A_n) = \Pr(A_1) \cdot \ldots \cdot \Pr(A_n)\). Note that if \(A \cap B = \emptyset\) then the two events are \textit{not independent}.  Note that if \(A, B\) are independent then \(A, B^c\) are also independent.

    \subparagraph*{Conditionally Independent: } 
        \(A_1, A_2, \ldots, A_k\) are \textit{conditionally independent} given \(B\) if, for every subset \(A_{i_1}, \ldots, A_{i_m}: \Pr(A_{i_1} \cap \ldots \cap A_{i_m} | B) = \Pr(A_{i_1} | B) \cdot \ldots \cdot \Pr(A_{i_m} | B)\).

    \subparagraph*{Bayes' Theorem:}
    \(\Pr(B_i|A)\\ = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\Pr(A)}
     = \frac{\Pr(A|B_i) \cdot \Pr(B_i)}{\sum_{j=1}^{k}\Pr(A|B_j) \cdot \Pr(B_j)}\)

    \subparagraph*{Uniform Distribution: }
        \(X = x, x \in \{1, 2, \ldots, k\}\) with all values \(x\) equally likely. The p.f. is \(f_X(x) = \frac{1}{k} \{x | x = 1, 2, \ldots, k\}\)
    

    

    \subparagraph*{Binomial:}
        \(n\) Bernoulli trials repeated independently with probability of success \(p\). 
        \begin{itemize}
            \item[] \(X :=\) number of success in \(n\) trials. 
            \item[] \(x \in \{0, 1, \ldots, n\}\). 
            \item[] \(f_X(x) = \Pr(X=x)\)
            \item[] \( =
    \begin{cases}
    {n \choose x} p^x (1-p)^{n-x}   &   x=0, \ldots, n \\
    0                               &   \text{o.w.}
    \end{cases} \)
            \item[] \(\rightarrow X \sim \text{Bin}(n,p)\)
        \end{itemize}

        \subparagraph*{Quantile Function: }
        \(X\) continuous r.v. \(F^{-1}(p)\) is the quantile function of \(X\) for \(0 \leq p \leq 1. F^{-1}(p) = x \Rightarrow p = F(x)\).

    \subparagraph*{Joint Continuous Distributions: }
    Joint p.d.f. given by \(f_{X,Y}(x,y) = \Pr((X,Y) \in A) = \iint{f(x,y)dxdy}\). To find the joint c.d.f. just integrate.

        \subparagraph*{Negative-Binomial: } Bernoulli trials until \(r\) successes are observed. \(X :=\) number of failures \(= \{0, 1, \ldots\}\). \(p :=\) probability of success. \(\Pr(X = x) = \Pr(x \text{ failures before \(r\) successes}) = \Pr(x\) failures, \(r-1\) successes,  \(x + r - 1\) trials\()) \cdot \Pr(\)one success in last trial\() = {x + r - 1 \choose x} (1 - p)^x p^r\).\\
        \(f_X(x) =
    \begin{cases}
    {x+r-1 \choose x} (1-p)^x p^r   &   x=0,1,2,\ldots \\
    0       & \text{o.w.}
    \end{cases}\)

    \subparagraph*{Hypergeometric: } A box with \(A\) red balls and \(B\) blue balls. \(n\) balls are drawn \textit{without replacement}. \(X :=\) number of red balls \(\leq \min(n, A)\). \(\max(n-B, 0) \leq X \leq \min(n,A)\). Bounds: \(\max(n-B,0) \leq x \leq \min(n,A)\\\rightarrow f_X(x) = 
    \begin{cases}
    \frac{ {A \choose x} \cdot {B \choose n - x}}{{A + B \choose n}}     & \text{bounds}    \\
    0   &   \text{o.w.}
    \end{cases}
    \)
        
    \subparagraph*{Poisson: }
        Counts occurences of an event. \(X\) is a Poisson r.v. with parameter \(\lambda\) (intensity) if the p.f. is \(f_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots \\ 0 & \text{o.w.} \end{cases}\) with \(\lambda>0\).

    \subparagraph*{Cumulative Distribution Function: } 
        (c.d.f.) For any r.v. \(X\) the c.d.f. is given by \(F(x) = \Pr(X \leq x)\). Properties: 
        If \(x_1 < x_2 \Rightarrow \{X \leq x_1 \} \subset \{X \leq x_2 \}\) and so \(\Pr(X \leq x_1) \leq \Pr(X \leq x_2) \Rightarrow F(x_1) \leq F(x_2) \\
        \lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty} F(x) = 1\)\\
        For a continuous r.v.: \\\(F(x) = \Pr(X \leq x) = \int_{-\infty}^{x}f(t)dt\) \\ 
    In general, \(X \sim Unif[a,b] \Rightarrow \\f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text{o.w.} \end{cases}\). 
    \\\(F(x) = \begin{cases} 0 & x < a \\ \frac{x-a}{b-a} & a \leq x \leq b \\ 1 & x > b \end{cases}\)



    \subparagraph*{Marginal Distributions: }
        In general for discrete r.v. \(f_X(x) = \sum_{y}f(x,y)\) and \(f_Y(y) = \sum_{x}f(x,y)\). In the case of 2 cont. r.v. \(f_X(x) = \int_{-\infty}^{\infty}f(x,y)dy\) and \(f_Y(y) = \int_{-\infty}^{\infty}f(x,y)dx\). \(f_X(x)\) is the marginal p.d.f. of \(X\). \(f_Y(y)\) is the marginal p.d.f. of \(Y\).

    \subparagraph*{Independence: }
        Two r.v. are independent if they produce independent events: \(\Pr(X \in A, Y \in B) = \Pr(X \in A) \cdot \Pr(Y \in B)\). This implies: \(\Pr(X \leq x, Y \leq y) = \Pr(X \leq x) \cdot \Pr(Y \leq y) \Rightarrow F(x,y) = F_X(x) \cdot F_Y(y)\).

    \subparagraph*{Conditional Distributions: }
        \(X, Y\) discrete r.v. with p.f. \(f_X(x), f_Y(y)\) and joint p.f. \(f(x,y)\). Then: \\
        \(\Pr(X = x | Y = y) = \frac{\Pr(X = x, Y = y)}{\Pr(Y = y)} = \frac{f(x,y)}{f_Y(y)}\). \\
        \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & \forall x,y: f_Y(y) > 0 \\ 0 & \text{o.w.}\end{cases} \\
        \sum_x{f_X(x|y)} = \sum_x{\frac{f(x,y)}{f_Y(y)}} \)
             \(= \frac{1}{f_Y(y)} \cdot \sum_x{f(x,y)} = \frac{f_Y(y)}{f_Y(y)} = 1\). \\
    In the continuous case \(X, Y\) with joint p.d.f. \(f(x,y)\) and marginal p.d.f.'s \(f_X(x)\) and \(f_Y(y)\): \\
    \(g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & f_Y(y) > 0 \\ 0 & \text{o.w.} \end{cases} \\
    \int\limits_{-\infty}^{\infty}g_X(x|y)dx = \int\limits_{-\infty}^{\infty} \frac{f(x,y)}{f_Y(y)}dx = \frac{1}{f_Y(y)} \int\limits_{-\infty}^{\infty}f(x,y)dx = \frac{1}{f_Y(y)} \cdot f_Y(y) = 1\)

\subparagraph*{Multivariate Distributions:}
 \(X_1, X_2, \ldots, X_n\) have a joint discrete distribution if \((X_, \dots, X_n)\) can have only a countable sequence of values in \(\mathcal{R}^n\). The joint p.f. is \(f(x_1,\dots,x_n) = \Pr(X_1=x_1, X_2=x_2,\ldots,X_n=x_n)\). \(X_1, X_2, \ldots, X_n\) have a joint continuous distribution if there exists \(f\) such that \(f\left((X_1,\ldots,X_n)\in \mathcal{C}\right) = \int\limits_{\mathcal{C}}\dots\int f(x_1,\ldots,x_n)dx_1\ldots dx_n\). Here \(f(x_1,\ldots,x_n)\) is the joint p.d.f. and \(f(x_1,\ldots,x_n) = \frac{d^nF(x_1,\ldots,x_n)}{dx_1\ldots dx_2}\). \(F(x_1,\ldots,x_n) = \Pr(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n)\).

 \subparagraph*{Marginal Distributions: }
    \(X_1, \ldots, X_n\) are cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Then the marginal distribution of \(X_1 = f_{X_1}(x_1) = \int\limits_{- \infty}^{\infty} \dots \int\limits_{- \infty}^{\infty} f(x_1,x_2, \ldots, x_n)dx_2dx_3\ldots dx_n\). \(F(x_1)\) is the marginal c.d.f. of \(X_1\) and \(F(x_1) = \Pr(X_1 \leq x_1) = \Pr(X_1 \leq x_1, X_2 < \infty, \ldots, X_n < \infty)\).

    \subparagraph*{Bernoulli: }
        An event \(A\) happens with probability \(p\): \(f_X(x) = p \text{ if } x = 1, (1-p) \text{ if } x = 0\)

    \subparagraph*{Geometric: }
        Negative binomial with \(r=1 \\
        f_X(x) = \begin{cases} {(1-p)}^x p & x=0,1,\ldots \\ 0 & \text{o.w.} \end{cases}\)


\subparagraph*{Conditional Distribution: }
 \(f(x_1, \ldots, x_n)\): joint p.d.f. of \(x_1, \ldots, x_n\); \(f_0(x_1, \ldots, x_k)\): joint p.d.f. of \(x_1, \ldots, x_k\) with \(k < n\). Then \(\forall x_1,\ldots, x_k\) such that \(f_0(x_1, \ldots, x_k) > 0\) the conditional p.d.f. of \(X_{k+1},\ldots,X_n\) given \(X_1 = x_1, \ldots, X_k = x_k\) is \(g(x_{k+1},\ldots,x_n | x_1, \ldots, x_k) = \frac{f(x_1, \ldots, x_n)}{f_0(x_1, \ldots, x_k)}\).

 \subparagraph*{Functions of one R.V: }
    Consider a r.v. \(X\) cont. with p.d.f. \(f_X(x)\). Assume we are interested in \(Y=r(X)\) with \(r\) a function. What is the dist. of \(Y\)? Let \(G_Y(y) = \Pr(Y \leq y)\) be the c.d.f. of \(Y\): \(G_Y(y) = \Pr(Y \leq y) = \Pr(r(X) \leq y) = \int{f(x)}dx \{x: r(x) \leq y\} \). To get the p.d.f. of \(Y\) take derivatives: \(g_Y(y) = \frac{dG_Y(y)}{dy} = \frac{1}{2} y^{-\frac{1}{2}}\). For continuous r.v. such that \(Y = r(X)\) with \(r\) differentiable and one-to-one: \(g_Y(y) = f_X(r^{-1}(y))\left|\frac{d}{dy} r^{-1} (y) \right|\).

\subparagraph*{Functions of two or more R.V.s:}
 Discrete case: \(X_1, X_2, \ldots, X_n\) r.v. with joint p.f. \(f(x_1, \ldots, x_n\):
\(Y_1 = r_1(X_1,\ldots,X_n)\) \dots \(Y_m = r_m(X_1,\ldots,X_n)\). Let \(A := \{(x_1,\ldots,x_n)\):= such that \(y_1 = r(x_1,\ldots,x_n) \dots y_m = r(x_1,\ldots,x_n)\)\}. Then: \(g(y_1,\ldots,y_m)=\Pr(Y_1=y_1,\ldots,Y_m=y_m)=\sum\limits_{(x_1,\ldots,x_n)\in A}{f(x_1,\ldots,x_n)}\). \\

Continuous case: \(X_1, X_2, \ldots, X_n\) cont. r.v. with joint p.d.f. \(f(x_1, \ldots, x_n)\). Let \(Y = r(X_1,\ldots,X_n) \rightarrow A_y = \{(x_1,\ldots,x_n)\) s.t. \(r(x_1,\ldots,x_n) \leq y\}\) Then the c.d.f. of \(Y\) is \(G(y) = \Pr(Y \leq y) = \Pr\left( r(X_1,\ldots,X_n) \leq y \right) = \int\limits_{A_y}\ldots\int{f(x_1,\ldots,x_n)}dx_1dx_2\ldots dx_n\). The density/p.d.f. of \(Y\) is \(g(y) = \frac{dG(y)}{dy}\). If \(Y = a_1 X_1 + a_2 X_2 + b \Rightarrow g(y) = \int_{-\infty}^{\infty}f\left(\frac{y-a_2 x_2 - b}{a_1},x_2\right)\left|\frac{1}{a_1}\right|dx_2\)

\subparagraph*{Permutations: } 
Given an array of \(n\) elements: \(n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 1 = n!\) \\
\(P_{n,k} = \frac{n!}{(n-k)!}\), \(P_{n,n} = n!\)

\subparagraph*{Combinations: }
In general we can ``combine'' \(n\) elements taking \(k\) at a time in\\ \(C_{n,k} = \frac{P_{n,k}}{k!} = \frac{n!}{(n-k)!k!} = {n \choose k} \). 

\subparagraph*{Multinomial Coefficients: } 
\(n\) elements into \(k (k \geq 2)\) groups s.t. group \(j\) gets \(n_j\) elements and \(\sum_{j = 1}^{k}n_j = n\). The \(n_1\) elements in the first group can be selected in \({n \choose n_1}\), the second in \({n-n_1 \choose n_2}\), the third in \({n-n_1-n_2 \choose n_3}\) and so on for the \(k\) groups. Then:\\ \({n \choose n_1}\cdot{n-n_1 \choose n_2}\cdot{n-n_1-n_2 \choose n_3}\cdot \ldots \cdot{n_k \choose n_k} = {n \choose n_1, n_2, \ldots, n_k}\)


\subparagraph*{Transformations: }
 \(X_1,\ldots,X_n\) cont. r.v.'s with joint pdf \(f(x_1,\ldots,x_n)\). Let \(Y_1 = r_1(X_1,\ldots,X_n), \dots, Y_n = r_n (X_1, \ldots, X_n)\). To find the joint pdf of \(Y_1, \ldots, Y_2\) for a one-to-one differentiable transformation \(x_1 = s_1 (y_1,\ldots,y_n), \dots, x_n = s_n (y_1, \ldots, y_n) \rightarrow \) The joint pdf of \(Y_1,\ldots,Y_n\) is \(g(y_1,\ldots,y_n) = f(s_1,\ldots,s_n)\left|J\right|\) where \(J = \det \begin{bmatrix}
    \frac{ds_1}{dy_1} & \dots & \frac{ds_1}{dy_n} \\
    \vdots & \ddots & \vdots \\
     \frac{ds_n}{dy_1} & \hdots & \frac{ds_n}{dy_n}
 \end{bmatrix}
 \)

 \subparagraph*{Linear Transformations: } Suppose that 
 \(\vec{X} = 
    \begin{pmatrix}
        X_1\\
        \vdots{}\\ 
        X_n
    \end{pmatrix}\) and 
\(\vec{Y} = 
    \begin{pmatrix}
        Y_1\\
        \vdots{} \\
        Y_n
    \end{pmatrix}
    = A\vec{X}\) (with \(A\) a non-singular matrix). Then \(\vec{X} = A^{-1}\vec{Y}\) and \(g_Y(y) = f_X(A^{-1})\cdot\frac{1}{\left|\det A\right|}\)

\subparagraph*{Markov Chains: }
A sequence of r.v.'s \(X_1, X_2, \ldots\) is a stochastic process with discrete time parameter. \(X_1\) is the initial state and \(X_n\) is the state at time \(n\). A stochastic process with discrete time parameter is a Markov chain if for each \(n\), \(\Pr(X_{n+1} \leq b | X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = \Pr(X_{n+1} = \leq b | X_n = x_n)\). A Markov Chain is finite if there are finite possible states. Then: \(\Pr(X_1 = x_1, \ldots, X_n = x_n) = \Pr(X_1 = x_1) \Pr(X_2 = x_2 | X_1 = x_1) \ldots \Pr(X_n = x_n | X_{n-1} = x_{n-1}) \). Transition distributions: When a MC has \(k\) possible states then it has a transition distribution where there exist probabilities \(p_{ij}\) for \(i, j = 1, \ldots , k\) such that \(\forall n: \Pr(X_{n+1} = j | X_n = i) = p_{ij}\) and if \(\Pr(X_{n+1} = j | X_n = i) = p_{ij} \forall n\) then it is a stationary transition distribution. In this case there is a matrix s.t. \\ \(\sum\limits_{j=1}^{k}{p_{ij}}=1,  \forall i\):
\(P = \begin{bmatrix} 
        p_{11} & \hdots & p_{1k} \\
        \vdots & \ddots & \vdots \\
         p_{k1} & \hdots & p_{kk}
     \end{bmatrix}\)\\     
Transition of several steps: \(P^m = P \cdot \ldots \cdot P\) Just exponentiate \(P\) and then find the resulting \(p_{ij}\).

\subparagraph*{Expectation: } \ \ \\
\( E(X) = \int_{-\infty}^{\infty}{x f(x)dx}\) or \(\sum\limits_{x}xf(x)\) \\
If \(Y = r(X)\) and \(f(x)\) is the p.d.f. of \(X: E(Y) = \int_{-\infty}^{\infty}{r(x)f(x)dx}\) \\
\(Y = aX + b \rightarrow E(Y) = aE(X)+b\) \\
\(a\) constant s.t. \(\Pr(X \geq a) = 1\) then \(E(X) \geq a\) \\
\(b\) constant s.t. \(\Pr(X \leq b) = 1\) then \(E(X) \leq b\). \\
If \(X_1, \ldots, X_n \) are r.v. then \(E(X_1+\ldots+X_n) = E(X_1) + \ldots +E(X_n)\) \\
\(E\left( \sum_{i = 1}^{n}X_i \right) = \sum_{i = 1}^{n}(E(X_i))\) \\ 
\(X_1, \ldots, X_n\) independent r.v.'s with finite expectation: \(E\left(\Pi_{i=1}^{n}{X_i} \right) = \Pi_{i=1}^{n}\left(E(X_i)\right)\) \\
Bernoulli(\(p\)): \(E(X) = p\) \\ 
Binomial(\(n,p\)): \(E(X) = np\) \\
Poisson: \(E(X) = \lambda\) \\ 
Geometric: \(E(X) = \frac{1-p}{p}\) \\
Negative Binomial: \(E(X) = \frac{r(1-p)}{p}\) \\
Hypergeometric: \(E(X) = \frac{nA}{A+B}\)

\subparagraph*{Variance: } \ \ \\
\(V(X) = E[(X-\mu)^x]\) with \(\mu = E(X)\) \\
S.D.: \(\sigma = \sqrt{V(X)}\) \\ 
\(V(X) \geq 0\)!!! \\ 
\(X\) discrete: \(V(X) = \sum\limits_X{(x-\mu)^2f(x)}\) \\ 
\(X\) cont.: \(V(X) = \int_{-\infty}^{\infty}{(x-\mu)^2f(x)dx}\) \\ 
\(V(X) = E[(X-\mu)^2] = E(X^2) - \mu^2\) \\ 
\(V(X) = 0 \iff \Pr(X = c) = 1\) \\
\(a, b\) constant: \(V(aX+b) = a^2V(X)\) \\ 
\(X_1,\ldots,X_n\) independent: \(V(X_1+\ldots+X_n) = V(X_1)+ \ldots + V(X_n)\) \\
Bernoulli: \(V(X) = p(1-p)\) \\ 
Binomial: \(V(X) = np(1-p)\) \\
Poisson: \(V(X) = \lambda\) \\
Geometric: \(V(X) = \frac{1-p}{p^2}\) \\
Negative Binomial: \(V(X) = \frac{r(1-p)}{p^2}\) \\
Hypergeometric: \(V(X) = \frac{nAB}{(A+B)^2} \cdot \frac{A+B-n}{A+B-1}\)

\subparagraph*{Conditional Variance: } \ \ \\
    \(V(Y|X=x) = E(Y^2|X=x) - [E(Y|X=x)]^2\\
    E(Y^2|X=x) = \int_{-\infty}^{\infty}{y^2 g(y|x) dy}\)

\subparagraph*{Covariance: }
\begin{itemize}
    \item[] Cov\((X,Y) = E[(X-\mu_X)(Y-\mu_Y)] = E(XY) - \mu_X \mu_Y \mu_Z\)
    \item[] Discrete: \(E(XY) = \sum\limits_x\sum\limits_yxyf(x,y)\)
    \item[] Discrete: \(\mu_X = E(X) = \sum\limits_x\sum\limits_yxf(x,y)\)
    \item[] Discrete: \(\mu_Y = E(Y) = \sum\limits_x\sum\limits_yyf(x,y)\)
    \item[] Cont: \(E(XY) = \int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}{xyf(x,y)dxdy}\)
    \item[] If \(X\) and \(Y\) are independent: Cov\((X,Y) = 0\)
\end{itemize}
\subparagraph*{Correlation: }
\begin{itemize}
    \item[] Corr\((X,Y) = \rho(X,Y)\)
    \item[] \(= \frac{\text{Cov}(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}} =\frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}\)
    \item[] Schwarz Ineq: \([E(UV)]^2 \leq E(U^2)E(V^2)\)
    \item[] \([\text{Cov}(X,Y)]^2 \leq V(X) \cdot V(Y)\)
    \item[] \( -1 \leq \rho(X,Y) \leq 1\)
    \item[] Indep: Cov\((X,Y) = 0\) and \(\rho(X,Y) = 0\)
    \item[] \(X\) r.v. w/ finite variance and \(Y = aX+b\) s.t. \(a \neq 0, a,b,\) constant, then
    \item[] \(a > 0 \rightarrow \rho(X,Y) = 1\) and
    \item[] \(a < 0 \rightarrow \rho(X,Y) = -1\)
    \item[] \(X,Y\) w/ finite var. then \(V(X+Y) = V(X)+V(Y) + 2\cdot \text{Cov}(X,Y)\)
    \item[] \(V(aX + bY) = a^2 V(X) + b^2 V(Y) + 2ab\cdot\text{Cov}(X,Y)\)
\end{itemize}

\subparagraph*{Conditional Expectation:}
\begin{itemize}
    \item[] Def: \(E(Y|X=x)\)
    \item[] Cont: \(= \int_{-\infty}^{\infty}{y g_Y(y|x)dy}\)
    \item[] Disc: \(= \sum_{y}{y g_Y(y|x)}\)
    \item[] \(E(Y|X=x)\) is a function of \(x\) not \(y\)
    \item[] \(h(x) = E(Y|x)\): \(h(x)\) is not a random variable
    \item[] \(E(Y|X) \neq E(Y|X=x)\)
    \item[] \(E(Y|X) = h(X) \rightarrow h(X)\) is a r.v.
    \item[] \(E(Y|X=x) = h(x) \rightarrow h(X)\) is not a r.v.
    \item[] \(E(E(Y|X)) = E(Y)\)
\end{itemize}

\subparagraph*{Standard Normal Distribution:}
\begin{itemize}
    \item[] \(X\) has standard normal dist. with \(\mu\) = 0 and \(V\) = 0, \(X \sim N(0,1)\) if:
    \item[] p.d.f.: \( \phi(x) = \frac{1}{\sqrt{2 \pi}}\exp\left (\frac{-x^2}{2}\right)\)
    \item[] c.d.f.: \( 
        \Phi(x) = \Pr(X \leq x) \)
    \item[] \(= \int_{-\infty}^{x}{\phi(u)du}= \int_{-\infty}^{x}{\frac{1}{\sqrt{2 \pi}} \exp\left(\frac{-u^2}{2}\right)du}\)
    \item[] \(\phi(x) = \phi(-x)\)
    \item[] \(\Phi(x) = \Pr(X \leq x) = 1 - \Phi(-x)\)
    \item[] \(\Phi^{-1}(p) = -\Phi(1-p)\)
    \item[] \(X \sim N(\mu, \sigma^2) \rightarrow Z = \frac{X-\mu}{\sigma}\)
    \item[] Then: \(Z \sim N(0,1)\) and cdf of \(X\) is
    \item[] \(F(x) = \Pr(X \leq x) = \Phi(\frac{x-\mu}{\sigma})\)
    \item[] Also: \(F^{-1}(p) = \mu + \sigma \Phi^{-1}(p)\)
    \item[] Linear combo of r.v. \(X_1, \ldots, X_n\) with \(X_i \sim N(\mu_i, \sigma_i^2)\) then:
    \item[] \(\sum\limits_{i = 1}^{n}{X_i} = X_1 + \ldots + X_n \sim N(\sum\limits_{i = 1}^{n} \mu_i, \sum\limits_{i = 1}^{n} \sigma_i^2)\)
    \item[] Generally: \(\sum\limits_{i = 1}^{n}{a_i X_i} = N(\sum\limits_{i = 1}^{n}{a_i \mu_i}, \sum\limits_{i = 1}^{n}{a_i^2 \sigma_i^2})\)
    \item[] Linear combo of r.v.'s: \(E(\bar{X_n}) =  \mu\)
    \item[] \(V(\bar{X_n}) = \frac{\sigma^2}{n}\)
    \item[] If \(Y\) is a linear combo of r.v.'s: \(Y \sim N(\mu, \sigma^2)\)
    \item[] To find \(\Pr(a < Y < b)\), use \(Z = \frac{Y-\mu}{\sigma}\)
    \item[] \(\Rightarrow Z \sim N(0,1)\)
    \item[] Markov Inequality: \(X\) r.v., \(\Pr(X \geq 0) = 1\), then: \(\forall t > 0: \Pr(X \geq t) \leq \frac{E(X)}{t}\)
    \item[] Chebyshev's Inequality: \(X\) r.v. w/ \(V(X) \Rightarrow \forall t > 0: \Pr(|X - E(X)| \geq t) \leq \frac{V(X)}{t^2}\)
    \item[] So for \(\bar{X_n}: \Pr(|\bar{X_n} - \mu|\geq t) \leq \frac{V(\bar{X_n}}{t^2} = \frac{\sigma^2}{nt^2}\)
\end{itemize}

\subparagraph*{Central Limit Theorem:}
\begin{itemize}
    \item[] \(X_1,\ldots,X_n\) i.i.d. sample from distribution with mean \(\mu\) and variance \(\sigma^2\). For each \(x (-\infty < x < \infty):\)
    \item[] \(\Rightarrow \lim_{n \to \infty} \Pr \left( \frac{ \bar{X_n} - \mu }{ \frac{ \sigma }{ \sqrt{n} } }  \leq x \right) = \Phi(x)\)
    \item[] with \(\Phi(x) =\) the c.d.f. of a standard normal
\end{itemize}

\subparagraph*{Min/Max:}
\(X_1, \ldots, X_n\) independent r.v.'s. \\
\(Y_1 = \min(X_i)\), \(Y_n = \max(X_i)\). \\
\(F(x) = \Pr(X_i \leq x) = F(y) \cdot \ldots \cdot F(y) = \left[F(y)\right]^n\)\\
\(G_n(y) = \Pr(\max\{X_i\} \leq y) = \left[F(y)\right]^n \Rightarrow \\ 
g_n(y) = \frac{d}{dy}G_n(y) = n\left[F(y)\right]^{n-1} \left(\frac{d F(y)}{dy}\right) \Rightarrow \\ 
g_n(y) = n\left[F(y)\right]^{n-1}f(y)\). And: \\ 
\(G_1(y) = Pr(\min\{X_i\} \leq y)  = 1 - \Pr(\min\{X_i\} > y) = 1 - \Pr(X_i > y) = 1 - \left[\left(1 - \Pr(X_i \leq y)\right)\right] = 1 - \left[\left(1 - F(y)\right) \cdot \ldots \cdot \left(1 - F(y)\right)\right] = 1 - \left(1 - F(y)\right)^n\\ \Rightarrow G_1(y) = 1-\left(1-F(y)\right)^n\\ \Rightarrow g_1(y) = \frac{d}{dy} G_1(y) = n\left[1 - F(y)\right]^{n-1}\left(f(y)\right)\)

    \subparagraph*{Other Stuff:}
    \begin{itemize}
        \item[] \(\sum\limits_{i=1}^{n}i = \frac{n(n+1)}{2},\sum\limits_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}\)
        \item[] \(\sum\limits_{i=0}^{n}c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1; \sum\limits_{i=0}^{\infty}c^i = \frac{1}{1-c}\)
        \item[] \(\frac{d}{dx}(a^x) = \ln a\)
        \item[] \((fg)' = f'g + fg'\)
        \item[] \(\left(\frac{f}{g}\right)' = \frac{f'g-fg'}{g^2}\)
        \item[] \(\frac{d}{dx}(f(g(x)))=f'(g(x))g'(x)\)
        \item[] \(\frac{d}{dx}(e^{g(x)})= g'(x)e^{g(x)}\)
        \item[] \(\frac{d}{dx}(\ln g(x)) = \frac{g'(x)}{g(x)}\)
        \item[] \(\int \frac{1}{x}dx = \ln |x| + c\), \(\int \frac{1}{ax+b}dx=\frac{1}{a}\ln |ax+b|+c\)
        \item[] \(\int e^u du = e^u + c\), \(\int {a^u}du = \frac{a^u}{\ln a} + c\)
        \item[] \(\int_{a}^{b}{f(g(x))g'(x)}dx \Rightarrow u=g(x) \Rightarrow \int_{g(a)}^{g(b)}f(u)du\)
        \item[] \(\int udv = uv - \int vdu\)
    \end{itemize}

    


    \end{multicols}

\end{document}

