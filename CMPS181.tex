% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,8pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{extsizes}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=0.25in, headheight=10pt, headsep=0pt]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{adjustbox}

\usepackage{wrapfig}
\usepackage{booktabs}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.25pc,itemsep=0em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}


\lstset{frame=single,
  language=c,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=fixed,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\newcommand{\code}{\lstinline}
\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.3in,left=.25in,right=.25in,bottom=.25in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rhead{Pete Wilcox | CruzID: pcwilcox | Student ID: 1593715}

\begin{document}
\footnotesize
\begin{multicols}{5}
    \setlength{\premulticols}{0pt}
    \setlength{\postmulticols}{0pt}
    \setlength{\multicolsep}{0pt}
    \setlength{\columnsep}{1pt}
    \begin{itemize}
        \item \code{ACID}
              \begin{itemize}
                  \item \code{ATOMICITY}: An atomic transaction happens as one unit, either the whole thing
                        commits or none of it does.
                  \item \code{CONSISTENCY}: A consistent transaction brings the DB from one valid state to
                        another valid state with respect to any constraints.
                  \item \code{ISOLATION}: Concurrent isolated transactions would have the same result if run sequentially.
                  \item \code{DURABILITY}: A committed transaction will remain committed even in the event of
                        a hardware failure.
              \end{itemize}
        \item \code{RAID Levels}
              \begin{itemize}
                  \item Level 0: No redundancy (just stripin)
                  \item Level 1: Mirrored (two identical copies)
                        \begin{itemize}
                            \item Each disk has an exact mirror image
                            \item Parallel reads; writes involve two disks
                            \item Maximum transfer rate = transfer rate of one disk
                        \end{itemize}
                  \item Level 0+1 (Level 10): Striping and Mirroring
                        \begin{itemize}
                            \item Parallel reads; writes involve two disks
                            \item Maximum transfer rate = aggregate bandwidth
                        \end{itemize}
                  \item Level 3: Bit-interleaved parity
                        \begin{itemize}
                            \item Striping Unit: one bit (or byte) (one check disk)
                            \item Each read and write request involves all disks; disk array can process one request at a time
                        \end{itemize}
                  \item Level 4: Block-interleaved parity
                        \begin{itemize}
                            \item Striping unit: one disk block (one check disk)
                            \item Parallel reads possible for small requests, large requests can utilize full bandwidth
                            \item Writes involve modified block \emph{and} check disk
                        \end{itemize}
                  \item Level 5: Block-interleaved distributed parity
                        \begin{itemize}
                            \item Similar to RAID level 4 but parity blocks are distributed over all disks
                        \end{itemize}
              \end{itemize}
        \item \code{Buffer Management in a DBMS}
              \begin{itemize}
                  \item DBMS maintains buffer pool of frames, each frame holds a page, info is in \code{<frame#, pageid>} table
                  \item Choice of frame replacement dictated by replacement policy such as LRU
                  \item When a page is requested:
                        \begin{itemize}
                            \item If requested page is not in pool:
                                  \begin{itemize}
                                      \item Choose a frame for replacement
                                      \item If that frame is dirty, write it to disk
                                      \item Read requested page into chosen frame
                                  \end{itemize}
                            \item Pin the page and return its address
                            \item When done the requestor must indicate whether the page has been modified (dirty bit) and unpin
                            \item Page in pool may be requested many times
                                  \begin{itemize}
                                      \item A pin count is used and a page is a candidate for replacement iff \code{pin_count = 0}
                                      \item Pinning increments pin count and unpinning decrements
                                  \end{itemize}
                            \item Concurrency control and recovery may entail additional I/O when a frame is chosen for replacement (write-ahead log protocol)
                            \item Frame is chosen for replacement using LRU, clock, MRU, etc
                            \item Sequential flooding: Caused by using LRU when the number of buffer frames is less than the number of pages in the file
                        \end{itemize}
              \end{itemize}
        \item \code{Files of Records}
              \begin{itemize}
                  \item Page or block is ok when doing I/O but higher levels of DBMS operate on \emph{records} and thus want \emph{files of records}
                  \item \code{FILE:} A collection of pages each containing a collection of records. Must support
                        \begin{itemize}
                            \item Insert (append)/delete/modify record
                            \item Read a particular record specified using \emph{record id}
                            \item Scan all records possibly with some conditions on the records to be retrieved
                        \end{itemize}
                  \item \code{Unordered ``Heap'' Files:}
                        \begin{itemize}
                            \item Simplest file structure that contains records in no particular (logical) order
                            \item As file grows and shrinks, disk pages are allocated and de-allocated
                            \item To support record-level operations we must:
                                  \begin{itemize}
                                      \item Keep track of the \emph{pages} in a file: \code{page id (pid)}
                                      \item Keep track of the \emph{free space} on a page
                                      \item Keep track of the \emph{records} on a page: \code{record id (rid)}
                                      \item Keep track of \emph{fields} within records
                                  \end{itemize}
                            \item Operations: create/destroy file, insert/delete record, fetch record with specific \code{rid}, scan all records
                        \end{itemize}
                  \item Record formats: \code{Fixed Length}
                        \begin{itemize}
                            \item Information about field types is the same for all records in file; it is stored in \emph{system catalogs}
                            \item Finding the $i^{th}$ field of a record does not require scanning the record
                        \end{itemize}
                  \item Record formats: \code{Variable length}
                        \begin{itemize}
                            \item Several alternative formats (\# of fields is fixed)
                            \item Fields delimited by special symbols (e.g. \$ between fields)
                            \item Fields preceded by lengths
                        \end{itemize}
                  \item  Record formats: \code{Variable length with directory}
                        \begin{itemize}
                            \item Use array of offsets at start of record
                        \end{itemize}
                  \item \code{Heap file implemented as a list}
                        \begin{itemize}
                            \item The header page id and heap file name must be stored someplace
                            \item Each page contains two extra pointers in this case
                            \item Refinement: use several lists for different degrees of free space
                        \end{itemize}
                  \item Page formats:
                        \begin{itemize}
                            \item File $\rightarrow$ collection of pages
                            \item Page -> collection of tuples/records
                            \item Query operators deal with tuples
                            \item Slotted page format:
                                  \begin{itemize}
                                      \item Each page has a collection of \emph{slots}
                                      \item Each slot contains a record
                                  \end{itemize}
                            \item RID: \code{<page id, slot number>}
                        \end{itemize}
                  \item \code{Heap file using a page directory}
                        \begin{itemize}
                            \item Page entries can include the number of free bytes on each page
                            \item Directory is a collection of pages; linked list is one possible implementation
                        \end{itemize}
                  \item \code{System catalogs:}
                        \begin{itemize}
                            \item For each relation:
                                  \begin{itemize}
                                      \item name, file, file structure
                                      \item name, type, length (if fixed) for each attribute
                                      \item Index name, target, and kind for each index
                                      \item also integrity constraints, defaults, nullability, etc
                                  \end{itemize}
                            \item For each index: structure (e.g. B+ tree) and search key fields
                            \item For each view: view name and definition (including query)
                            \item Plus statistics, authorization, buffer pool size, etc
                        \end{itemize}
                  \item \code{Column Stores:}
                        \begin{itemize}
                            \item Store data ``vertically''
                            \item Contrast with a ``row-store'' that stores all the attributes of a tuple/record contiguously
                            \item Each column can be stored as a separate file and compressed
                            \item SAP HANA:
                                  \begin{itemize}
                                      \item Dictionary compression per column
                                      \item Column main: read-optimized store for immutable data. Uses high data compression and heuristic algoriths to order data to maximize secondary compression
                                      \item Column delta: write-optimized store for inserts, updates, deletes. Uses less compression, appends updates to the end, and merges with main periodically.
                                  \end{itemize}
                            \item Additional types: prefix coding, run length coding, cluster coding, sparse coding, indirect coding
                        \end{itemize}
              \end{itemize}
        \item \code{Indexes:}
              \begin{itemize}
                  \item Speeds up selections on the search key fields for the index
                  \item Contains a collection of data entries and supports efficient retrieval of all data entires $k^*$ with a given key value $k$
              \end{itemize}
        \item \code{B+ Tree Indexes}
              \begin{itemize}
                  \item Leaf pages contain \emph{data entries} and are chained (prev \& next)
                  \item Non-leaf pages have \emph{index entries}, used to direct searches
                  \item Insert/delete at $\log_F N$, keep tree \emph{height-balanced} ($F = $ fanout, $N = $ \# leaf pages)
                  \item Minimum 50\% occupancy (in all nodes except root). Each node contains $d \leq m \leq 2d$ entries; $d = $ the \emph{order} of the tree.
                  \item Typical order $d = 100$
                  \item Percentage of node that is full is more useful, typical fill-factor 67\%
                  \item Average \emph{fanout} for non-leaves $F = 133$
                  \item Inserting a data entry:
                        \begin{itemize}
                            \item Find correct leaf $L$
                            \item Put data entry onto $L$
                            \item If $L$ has enough space, done
                            \item Otherwise, must split $L$. Redistribute entries evenly, copy up the middle key (key must still exist in leaf). Insert index entry pointing to $L_2$ into parent of $L$.
                            \item This can happen recursively: if parent of $L$ grows, need to push up middle key.
                            \item Splits ``grow'' the tree; root split increases height.
                        \end{itemize}
                  \item Deleting a data entry:
                        \begin{itemize}
                            \item Start at root, find leaf $L$ where entry belongs
                            \item Remove the entry
                            \item If $L$ is at least half full, done
                            \item Otherwise, if $L$ has only $d-1$ entries, try to redistribute, borrowing from sibling (adjacent node with same parent)
                            \item If redistribution fails, merge $L$ and sibling
                            \item If merge occurred, must delete entry from parent (pointing to merged node)
                            \item Merge can propagate to root, decreasing height of the tree
                        \end{itemize}
              \end{itemize}
        \item \code{Hash-Based Indexes:}
              \begin{itemize}
                  \item Good for equality selections
                  \item Index is a collection of \emph{buckets}. Each bucket = \emph{primary page} plus zero or more \emph{overflow pages} (called \emph{static} hashing). Buckets contain data entries.
                  \item \emph{Hashing function} $h$: $h(r) = $ bucket in which (data entry for) record $r$ belongs. $h$ looks at the \emph{search key} fields of $r$.
              \end{itemize}
        \item Alternatives for Data Entry $k^*$ in index:
              \begin{itemize}
                  \item In a data entry $k^*$ we can store: an actual data record, or \code{<k, RID>}, or \code{<k, list of RIDs>}
                  \item Choice of alternative for entries is orthogonal to the indexing technique
              \end{itemize}
        \item Alternative 1: data records live in index
              \begin{itemize}
                  \item Index structure is actually a file organization for the data records
                  \item At most one index on a given collection of data can use this Alternative
                  \item If data records are very large, \# of leaf pages containing data entries is high.
              \end{itemize}
        \item Alternatives 2 and 3: Key/RID or Key/RIDlist:
              \begin{itemize}
                  \item Data entries are typically much smaller than data records
                  \item Alternative 3 is more compact but leads to variable-sized data entries, even if the search keys are of fixed length
              \end{itemize}
        \item Index classification:
              \begin{itemize}
                  \item \emph{Primary vs Secondary:} if search key contains the primary key, index is called the primary index
                  \item \emph{Clustered vs Unclustered:} If order of data records is the same as (or close to) the order of stored data records then index is called a clustered index.
              \end{itemize}
        \item A back of the envelope cost model:
              \begin{itemize}
                  \item $B$: the number of data pages
                  \item $R$: number of records per page
                  \item $D$: average time to read or write a disk page
                  \item $F$: average fanout for a non-leaf page
              \end{itemize}
        \item Indexes with composite search keys:
              \begin{itemize}
                  \item Composite search keys: search on a combination of fields
                  \item Equality query: every field value is equal to a constant value
                  \item Range query: some field value doesn't have equality test
                  \item Data entries in index sorted by search key to support range queries
              \end{itemize}
        \item \code{ISAM:} Index-Sequential Access Method
              \begin{itemize}
                  \item Index file has first key on each page, can binary search index then scan the page.
                  \item \emph{Static} structure, inserts and deletes only affect leaf or overflow pages.
                  \item If index is very large, recursively create a second layer (and so on).
                  \item \emph{File Creation:} Leaf pages first allocated sequentially, sorted by search key; then index pages allocated, and then overflow pages.
                  \item \emph{Index entries:} \code{<key value, page id>}; they `direct' searches for \emph{data entries} which are in leaf pages
                  \item \emph{Search:} Start at root; use key comparisons to go to leaf. I/O cost $\propto \log_F N$ where $F = $ \# entries/index pg, $N = $ \# leaf pgs
                  \item \emph{Insert:} Find leaf where data entry belongs and put it there, using overflow page if necessary.
                  \item \emph{Delete:} Finda nd remove from leaf; if empty overflow page, deallocate
              \end{itemize}
        \item \code{B-Tree Prefix Key Compression:} Increase fan-out by reducing the size of search keys on interior nodes. key values only direct traffic so we only need the minimum length for that
        \item \code{Bulk Loading of a B+ Tree}
              \begin{itemize}
                  \item Creating a new B+ tree by inserting one at a time is very slow, bulk loading is better
                  \item \emph{Initialization:} Sort all data entries, insert pointer to first (leaf) page in a new (root) page
                  \item Index entries for leaf pages always entered into right-most index page just above leaf level. When this fills up it splits.
              \end{itemize}
        \item \code{Log-Structured Merge Tree:} Sequential trees of exponentially larger size. Inserts go to smallest smallest tree, deletes insert tombstone records, spill to next-deeper level on overflow
        \item \code{R-Tree:} Tree of rectangles, search for intersections between them
        \item \code{Static Hash-based Indexes:}
              \begin{itemize}
                  \item \# primary pages is fixed, allocated sequentially, never de-allocated; overflow pages if needed
                  \item $h(k) \mod M = $ bucket(page) to which data entry with key $k$ belongs ($M = $ \# buckets)
                  \item Buckets contain data entries
                  \item Hash function works on \emph{search key field} of record $r$. Must distribute over range $0 \dots M-1$
                  \item $h(key) = (a * key + b)$ usually works well; $a$ and $b$ are constants to tune $h$
                  \item \emph{Long overflow chains} can develop and degrade performance.
              \end{itemize}
        \item \code{Extendible Hashing:}
              \begin{itemize}
                  \item Situation: bucket (primary page) becomes full. Solved by doubling number of buckets instead of using an overflow page
                  \item Use directory wich pointers to buckets. Double the number of buckets by doubling the directory and splitting buckets as needed
                  \item Only one bucket at a time splits. No overflow pages
                  \item \emph{Global Depth} is the last $d$ bits after hashing and indexes into the directory to determine which bucket is used
                  \item \emph{Local Depth} is used for each bucket. If the $LD == GD$ and the bucket splits, the directory must double.
              \end{itemize}
        \item \code{Bitmap Indexes:}
              \begin{itemize}
                  \item Index which allows for fast equality checks. Order the records in some $O(1)$ way and maintain one or more bit vectors storing their values for particular fields.
                  \item One bitmap for each distinct domain value, and one bitmap for \code{NULL} if the column can be null.
                  \item Can use \code{XOR} operation to reduce number of maps needed by one.
              \end{itemize}
        \item \code{External Sorting:}
              \begin{itemize}
                  \item Goal: Need to sort more data than will fit in memory, efficiently.
                  \item \code{2-Way Sort:} Requires 3 buffers
                        \begin{itemize}
                            \item Pass 0: read a page, sort it, write it out (only one buffer page used)
                            \item Pass 1, 2, 3, ...: Read and merge pairs of runs. (Three buffer pages are used)
                        \end{itemize}
                  \item Sorting $N=2^k Pages of Data$:
                        \begin{itemize}
                            \item Pass 0: read, sort, write $\rightarrow 2^k$ 1-page runs
                            \item Pass 1: Read + merge 1-page pairs, write $\rightarrow 2^{k-1}$ 2-page runs
                            \item Pass 2: Read + merge 2-page pairs, write $\rightarrow 2^{k-2}$ 4-page runs
                            \item Pass $k-1$: Read + merge $2^{k-2}$-page pairs, write $\rightarrow 2 2^{k-1}$-page runs
                            \item Pass $k$: Read + merge $2^{k-1}$-page pairs, write $\rightarrow 1 2^k$-page result
                        \end{itemize}
                  \item 2-Way External merge sort: $N$ pages in file $\implies \lceil \log_2 N \rceil + 1$ passes, total I/O cost $= 2 N \left ( \lceil \log_2 N \rceil + 1 \right )$
                  \item \emph{General} external merge sort:
                        \begin{itemize}
                            \item Sorting a file with $N$ pages using $B$ buffer pages.
                            \item Pass 0: use $B$ buffer pages. Produce $\lceil N / B \rceil$ sorted runs of $B$ pages each.
                            \item Pass 2, ... etc: merge $b - 1$ runs
                            \item Number of passes: $1 + \left \lceil \log_{B-1} \lceil N / B \rceil \right \rceil $
                            \item Cost $= 2N * ($\# of passes $)$
                        \end{itemize}
                  \item \emph{Double Buffering}
                        \begin{itemize}
                            \item To reduce wait time for I/O request to complete, can \emph{prefetch} into \code{shadow block}
                            \item Potentially more passes; in practice, most files still sorted in 2-3 passes.
                        \end{itemize}
                  \item \emph{B+ Tree as ``Sorted Access Path''}
                        \begin{itemize}
                            \item Scenario: table to be retrieved in some order has a B+ tree index on the ordering columns
                            \item Idea: retrieve records in order by traversing the B+ tree's leaf pages
                            \item Very good idea if the tree is clustered, otherwise probably a bad idea
                        \end{itemize}
              \end{itemize}
        \item \code{Query Processing}
              \begin{itemize}
                  \item \emph{Access Paths:}
                        \begin{itemize}
                            \item An access path is a method of retrieving tuples: file scan or index that matches a selection in the query
                            \item A tree index \emph{matches} (a conjunction of) terms that involve only attributes in a \emph{prefix} of the search key.
                            \item A hash index \emph{matches} (a conjunction of) terms that has a term \emph{attribute = value} for every attribute in the search key of the index.
                        \end{itemize}
                  \item Selection conditions often first converted to be in CNF (\code{AND}ing of \code{OR}s)
                  \item One approach to selections:
                        \begin{itemize}
                            \item Find the \emph{most selective access path}, retrieve tuples using it, then apply any remaining terms which don't match the index
                            \item Most selective access path: an index or file scan that we estimate will require the fewest page I/Os
                            \item Terms that match this index reduce the number of tuples retrieved; other terms used to filter the retrieved tuples on the fly, but don't prevent retrieval of the tuples/pages
                        \end{itemize}
                  \item Using an index for selections: cost depends on \# qualifying tuples and clustering
                        \begin{itemize}
                            \item Cost of finding qualifying data entries (typically small) plus cost of retrieving the actual records themselves (can be large without clustering)
                        \end{itemize}
                  \item \emph{Duplicate Elimination}
                        \begin{itemize}
                            \item Relational algebra projection removes duplicates: SQL systems don't remove duplicates unless the keyword \code{DISTINCT} is specified
                            \item Sorting approach: sort on \code{<sid, bid>} and remove duplicates. Can optimize by dropping unwanted columns while sorting.
                            \item Hashing approach: hash on \code{<sid, bid>} to create \emph{partitions}. Load partitions into memory one at a time, build an in-memory hash structure, and eliminate duplicates within it.
                        \end{itemize}
                  \item Notation:
                        \begin{itemize}
                            \item Pages in a heap relation $R$: Pages$_R$
                            \item Tuples per page for a relation $R$: TPP$_R$
                            \item Number of tuples in $R$: Card$_R$
                            \item Card$_R = $ Pages$_R * $ TPP$_R$
                        \end{itemize}
                  \item \emph{Simple Nested Loops Join}: foreach tuple in $R$, for each tuple in $S$, if $r_i == s_j$ then add \code{<r,s>} to result.
                        \begin{itemize}
                            \item For each tuple in the \emph{outer} relation $R$ we scan the entire \emph{inner} relation $S$. Cost: Pages$_R$ + Card$_R * $ Pages$_S$
                            \item Page-oriented nested loops join: for each \emph{page} of $R$ get each \emph{page} of $S$ and write out matching pairs of tuples
                        \end{itemize}
                  \item \emph{Index Nested Loops}
                        \begin{itemize}
                            \item If there is an index on the join column of one relation, can make it the inner and exploit the index. Cost: Pages$_R$ + Card$_R$ * cost of finding matching $S$ tuples
                            \item For each $R$ tuple, cost of probing $S$ index is about 1.2 for hash index, 2-4 for B+ tree. Cost of then finding $S$ tuples (assuming alt. 2 or 3) depends on clustering. Clustered typically 1 I/O, unclustered up to 1 I/O per matching $S$ tuple.
                        \end{itemize}
                  \item \emph{Block Nexted Loops Join}
                        \begin{itemize}
                            \item Use one page as an input buffer for scanning the inner $S$, one pas as the output buffer, and use \emph{all} remaining pages to hold ``block'' of pages of outer $R$.
                            \item For each block of $R$, hash each data entry to a hash table. Then compare all entries in $S$.
                        \end{itemize}
                  \item \emph{Join: Sort-Merge } $\left (R \bowtie_{i=j} S \right)$
                        \begin{itemize}
                            \item Sort $R$ and $S$ on the join column, then scan them to do a `merge' (on join column) and then output result tupls.
                            \item $R$ is scanned once; each $S$ group is scanned once per matching $R$ tuple.
                        \end{itemize}
                  \item \emph{Statistics and Catalogs}
                        \begin{itemize}
                            \item Catalogs typically contain at least: \emph{\# tuples} and \emph{\# pages} in each relation; \emph{\# distinct key values} and \emph{\# pages} for each index; \emph{index height}, \emph{low/high key values} for each tree index
                            \item Catalogs updated periodically: updating each time data changes is too expensive and approximation is fine
                        \end{itemize}
                  \item \emph{Cost estimation:} For each plan must estimate cost
                        \begin{itemize}
                            \item Estimate \emph{Cost} of each operation in plan-tree: depends on input cardinalities
                            \item Estimate \emph{Size} of result for each operation in the tree; for seelctions and joins assume independence of predicates
                        \end{itemize}
                  \item \emph{Size estimation and reduction factors}
                        \begin{itemize}
                            \item Maximum \# of tuples in result is the product of cardinalities in the \code{FROM} clause
                            \item Reduction Factor $RF$ associated with each \emph{term} reflects the impact of \emph{term} in reducing result size
                        \end{itemize}
                  \item \emph{Grace Hash-Join}
                        \begin{itemize}
                            \item Like a two-phase index nested loop join
                            \item \emph{Build phase:} Partition both relations using hash function $h$: $R$ tuples in partition $i$ will only match $S$ tuples in partition $i$.
                            \item \emph{Match phase:} Read in a partition of $R$, hash it using $h_2 (<>h)$. Scan matching partition of $S$ searching for its $R$ matches.
                            \item \# partitions $k \leq B-1$ and Pages$_R /k$ (size of largest partition to be held in memory) $< B-1$.
                            \item Assuming uniformy-sized partitions and maximizing $k$: $k = B-1$ and Pages$_R /k < B-1$ so $(B-1)^2 > $Pages$_R \implies B > \sqrt{\text{Pages}_R}$
                            \item Can hash-join \emph{recursively} to reduce the amount of memory needed
                            \item In build phase: Read + write both relations: $2*($Pages$_R + $Pages$_S)$
                            \item In match phase: Read both relations; Pages$_R + $Pages$_S$ I/Os
                        \end{itemize}
                  \item Sort-Merge Join vs Hash Join:
                        \begin{itemize}
                            \item Given a a reasonable amount of memory $(B > \sqrt{\text{Pages}_R})$, both have cost $3*(\text{Pages}_R + \text{Pages}_s)$ I/Os
                            \item Hash join is superior is relation sizes differ greatly because it needs less memory, and is also highly parallelizable
                            \item Sort-Merge is less sensitive to data skew and its result is sorted
                        \end{itemize}
                  \item \emph{More general join conditions:}
                        \begin{itemize}
                            \item Equalities over several attributes: for INL, build index on composite key. For sort-merge and hash join, sort/hash-partition on the combination of the join columns
                            \item Inequality conditions: for INL, need (clustered!) B+ tree index; hash-join not usable; merge-join possible; block NL the best
                        \end{itemize}
              \end{itemize}
        \item \code{Two Approaches to General Selections}
              \begin{itemize}
                  \item \emph{First Approach:} Find the \emph{most selective access path}, retrieve tuples using it, and apply any remaining terms which don't match the index
                        \begin{itemize}
                            \item \emph{Most selective access path}: an index or file scan that we estimate will require the fewest page I/Os
                            \item Terms that match this index reduce \# of tuples \emph{retrieved}; other terms are used to \emph{discard} some retrieved tuples but do not reduce number of pages read
                        \end{itemize}
                  \item \emph{Second approach:} Intersection of RIDs
                        \begin{itemize}
                            \item Get sets of RIDs for data records using each matching index
                            \item Then \emph{intersect} these sets
                            \item Finally retrieve the records and apply any remaining terms
                        \end{itemize}
              \end{itemize}
        \item \code{The Projection Operation}
              \begin{itemize}
                  \item An approach based on \emph{sorting}:
                        \begin{itemize}
                            \item Modify pass 0 of external sort so that it also elminates unwanted fields. Thus, runs of pages are produced, but tuples in runs are smaller than input tuples.
                            \item Modify merging passes to eliminate duplicates (!). Thus, \# of result tuples is smaller than number of tuples in input.
                            \item Cost: in pass 0, read original relation ($M$ pages), write out same tuples but \emph{fewer columns}. In merging passes, \emph{fewer tuples} written out in each pass.
                        \end{itemize}
                  \item Projection based on \emph{hashing}:
                        \begin{itemize}
                            \item \emph{Partitioning phase:} Read $R$ using one input buffer. For each tuple, discard unwanted fields and use hash function $h_1 ($tuple$)$ to pick one of the $B-1$ output buffers.
                            \item \emph{Duplicate elimination phase:} For each partition, read it in, and build an in-memory hash table with hash function $h_2 \neq h_1$ on ``wanted'' fields while discarding duplicates (!).
                            \item \emph{Cost:} For partitioning, read $R$, write out each tuple, but with fewer fields. Less data read in next phase.
                        \end{itemize}
              \end{itemize}
        \item \code{Relational Set Operations}
              \begin{itemize}
                  \item Intersection and Cross-Product are special cases of Join.
                        \begin{itemize}
                            \item Intersection does Join matching \emph{all columns} in join predicates
                            \item Cross-Product does Join matching \emph{no columns} in join predicates
                        \end{itemize}
                  \item Union (which is \code{DISTINCT} not \code{ALL}) and Except are similar, here is a sorting-based approach to Union:
                        \begin{itemize}
                            \item Sort both relations (on combination of all attributes)
                            \item Scan sorted relations and merge them, discarding duplicates
                            \item \emph{Alternative}: merge runs from pass 0 for \emph{both} relations (!) discarding duplicates.
                        \end{itemize}
                  \item Hash-based approach to Union (from Grace)
                        \begin{itemize}
                            \item Partition both $R$ and $S$ using hash function $h_1$
                            \item For each $S$-partition build in-memory hash table using $h_2$, then scan corresponding $R$-partition, adding truly new $S$ tuples to hash table while discarding duplicates
                        \end{itemize}
              \end{itemize}
        \item Aggregate operations (\code{AVG}, \code{MIN}\ldots)
              \begin{itemize}
                  \item Without grouping: In general, requires scanning the full relation. Given an index whose search key includes all attributes in the \code{SELECT}/\code{WHERE} clauses, can do an index-only scan
                  \item With grouping:
                        \begin{itemize}
                            \item Sort on group-by attributes, then scan relation and compute the aggregate for each group.
                            \item Or, similar approach using hashing
                            \item Given tree index whose search key includes all attributes in \code{SELECT}, \code{WHERE}, \code{GROUP BY} clauses, can do an index-only scan
                        \end{itemize}
              \end{itemize}
        \item \code{Query Optimization}
              \begin{itemize}
                  \item \emph{Query blocks}: an SQL query is parsed into a collection and they are optimized one block at a time
                  \item Nested subquery blocks are usually treated as calls to a subroutine
                  \item For each block, plans considered are:
                        \begin{itemize}
                            \item All available access methods for each relation in the \code{FROM} clause
                            \item All \emph{left-deep join trees} i.e. all ways to join the relations one-by-one
                            \item Access an initial relation as outer, then take the next (inner) relation in the \code{FROM} clause, considering all relation permutations and join methods
                        \end{itemize}
                  \item \emph{Relational Algebra Equivalences} allow us to choose different join orders and to `push' selections and projects ahead of joins
                        \begin{itemize}
                            \item \code{Selections:} $\sigma_{c_1 \land \dots \land c_n} (R) = \sigma_{c_1} ( \ldots \sigma_{c_n} (R))$
                            \item \code{Selections:} $\sigma_{c_1}(\sigma_{c_2}(R)) = \sigma_{c_2}(\sigma_{c_1}(R))$
                            \item \code{Projections:} $\pi_{a_1 \land \dots \land a_n}(R) = \pi_{a_1}(\dots (\pi_{a_n}(R)))$
                            \item \code{Joins:} $R \bowtie (S \bowtie T) = (R \bowtie S) \bowtie T$
                            \item \code{Joins:} $(R \bowtie S) = (S \bowtie R)$
                            \item A projection commutes with a selection that only uses the attributes retained by the projection
                            \item Selection between attributes of the two arguments of a cross-product converts the cross-product to a join
                            \item A selection on just attributes of $R$ commutes with $R \bowtie S$: $\sigma(R \bowtie S) = \sigma(R) \bowtie S$
                            \item Similarly if a projection follows a join $R \bowtie S$ we can push parts of the projection into $R$ and $S$
                        \end{itemize}
                  \item Cost Estimates for Single-Relation Plans:
                        \begin{itemize}
                            \item Index $I$ on primary key matching selection: cost is height of $I+1$ for a B+ tree, about 1.2 for a hash index
                            \item Clustered index $I$ matching one or more selects: $(Pages_I + Pages_R) *$ product of RF's of matching selects
                            \item Non-clustered index $I$ matching one or more selects: $(Tuples_R) *$ product of RF's of matching selects
                            \item Sequential scan of file: $Pages_R$
                        \end{itemize}
                  \item \emph{Enumeration of Left-Deep Plans}
                        \begin{itemize}
                            \item Left-Deep plans differ only in the order of relations, the access method, and the join method
                            \item Enumerated using N stages: stage 1 finds best plan for each relation, stage 2 finds best plan joining result of each 1-relation plan to another relation (all 2-relation plans), and so on
                            \item For each subset of relations, retain only the cheapest plan overall and the cheapest plan for \emph{interesting order} of tuples
                        \end{itemize}
                  \item \emph{Interesting orders:} A given data order is \emph{interesting} if it has the potential to save work later on
                        \begin{itemize}
                            \item Ordering on Join attributes
                            \item Ordering on \code{GROUP BY} attributes
                            \item Ordering on \code{DISTINCT} attributes
                            \item Ordering on \code{ORDER BY} attributes
                        \end{itemize}
                  \item A partial plan on $k$ relations is combined with an additional relation only if there is a join condition between them, except if all join predicates in the \code{WHERE} clause combining the $k$ relations with another relation have been used up
              \end{itemize}
        \item Intermediate result size estimation:
              \begin{itemize}
                  \item For each relation $R$: Cardinality $|R|$, avg $R$-tuple width, and \# of pages in $R$
                  \item For each indexed attribute of $R$:
                        \begin{itemize}
                            \item Number of distinct values $|\pi_A(R)|$
                            \item Range of values (low to high)
                            \item Number of index leaf pages
                            \item Number of index levels (if B+ tree)
                        \end{itemize}
              \end{itemize}
        \item Simple selection queries $\sigma_P$
              \begin{itemize}
                  \item Equality predicate ($p$ is ``$A = val$'')
                        \begin{itemize}
                            \item $|Q| \approx |R| / |\pi_A(R)|$
                            \item $R$'s cardinality divided by the number of distinct $A$ values, assumes all values equally likely
                        \end{itemize}
                  \item Range predicate ($p$ is ``$val_1 \leq A \leq val_2$'')
                        \begin{itemize}
                            \item $|Q| \approx |R| * ((val_2 - val_1 + 1) / ((high(R.A) - low(R.A))))$
                            \item Selected range size divided by full range size, assumes all values equally likely
                        \end{itemize}
                  \item Boolean selection predicates
                        \begin{itemize}
                            \item Conjunctive (``$p_1$ and $p_2$''): $RF_p \approx RF_{p_1} * RF_{p_2}$
                            \item Negative (``not $p_1$''): $RF_p \approx 1 - RF_{p_1}$
                            \item Disjunctive (``$p_1$ or $p_2$''): $RF_p \approx RF_{p_1} + RF_{p_2} - (RF_{p_1} * RF_{P_2})$
                        \end{itemize}
              \end{itemize}
        \item Two-way Equijoin Predicates
              \begin{itemize}
                  \item Query $Q$: $R$ join $S$ on $R.A = S.B$
                  \item Assume \emph{join value set containment} (foreign key/primary key)
                  \item $\pi_A (R)$ is a subset of $\pi_B (S)$ or vice versa
                  \item $|Q| \approx (|R| * |S|) / \max{(|\pi_A(R)|, |\pi_B(S)|)}$
              \end{itemize}
        \item Lock types:
              \begin{itemize}
                  \item $S$ is a read-lock and is compatible with other $S$ locks
                  \item $X$ is a write-lock and is incompatible with all locks
              \end{itemize}
        \item \code{TRANSACTION SCHEDULING}
              \begin{itemize}
                  \item A \emph{serializable schedule} is equivalent to a serial schedule of \emph{committed} transactions
                  \item \emph{Cascading Rollbacks:} If transaction $T_2$ is dependent on data written by $T_1$, and $T_1$ is rolled back, $T_2$ must be rolled back as well.
                  \item \emph{ACR Avoid Cascading Rollbacks} schedule prevents this
                  \item \code{TWO-PHASE LOCKING 2PL}
                        \begin{itemize}
                            \item If transaction $T$ wants to read/modify an object it first obtains an $S$ or $X$ lock
                            \item If $T$ releases a lock it can acquire no new locks
                            \item Guarantees serializability
                            \item \emph{Strict 2PL}: hold all locks until the commit point, guarantees ACR
                        \end{itemize}
                  \item Thm: A schedule is conflict serializable iff its precedence graph is acyclic.
                  \item Thm: 2PL ensures that the precedence graph will be acyclic.
              \end{itemize}
        \item \code{ISOLATION LEVELS}
              \begin{itemize}
                  \item \code{Serializable}: default, long-term R/W locks on phantoms too
                  \item \code{Repeatable read}: long-term R/W locks on real objects; read only committed records, between two reads by the same transaction, no updates by another transaction
                  \item \code{Read committed}: long-term W locks, short-term R locks; read only committed records
                  \item \code{Read uncommitted}: read ignoring locks!
              \end{itemize}
        \item \code{WRITE-AHEAD LOGS}
              \begin{itemize}
                  \item Append to the log any time a transaction updates a record with the `before' and `after' values
                  \item Append to the log any time a transaction commits
                  \item Write the log to disk before writing the data
                  \item Create a \emph{checkpoint} periodically to minimize crash recovery time; includes a description of transactions that were active and (possibly) dirty pages at time it started
              \end{itemize}
        \item \code{CRASH RECOVERY}: 3 phases
              \begin{itemize}
                  \item \emph{Analysis}: read the most recent checkpoint, scan log forward to id all transactions that were active + dirty pages in buffer pool
                  \item \emph{Redo}: write out all dirty pages in buffer pool and redo all updates in log by applying \emph{after values} of their updates
                  \item \emph{Undo}: Undo all the writes of transactions which were active at the crash as well as all rolled by transactions
              \end{itemize}
        \item Indexes:
              \begin{itemize}
                  \item Terms:
                        \begin{itemize}
                            \item $B$: the number of data pages
                            \item $R$: number of records per page
                            \item $D$: average time to read or write a page
                            \item $F$: average fanout for a non-leaf page
                            \item $P$: \# matching pages
                        \end{itemize}
                  \item \code{Heap}
                        \begin{itemize}
                            \item Scan: $BD$
                            \item Equality: $0.5 BD$
                            \item Range: $BD$
                            \item Insert: $2D$
                            \item Delete: Search + $D$
                        \end{itemize}
                  \item \code{Sorted}
                        \begin{itemize}
                            \item Scan: $BD$
                            \item Equality: $D \log_2 B$
                            \item Range: $D(\log_2 B + P)$
                            \item Insert: Search + $BD$
                            \item Delete: Search + $BD$
                        \end{itemize}
                  \item \code{Clustered}
                        \begin{itemize}
                            \item Scan: $1.5 BD$
                            \item Equality: $D \log_F 1.5 B$
                            \item Range: $D(\log_F 1.5B + P)$
                            \item Insert: Search + $D$
                            \item Delete: Search + $D$
                        \end{itemize}
                  \item \code{Unclust. Tree}
                        \begin{itemize}
                            \item Scan: $BD(R+0.15)$
                            \item Equality: $D(1 + \log_F 0.15 B)$
                            \item Range: $D(\log_F 0.15B + P)$
                            \item Insert: Search + $2D$
                            \item Delete: Search + $2D$
                        \end{itemize}
                  \item \code{Unclust. Hash}
                        \begin{itemize}
                            \item Scan: $BD(R+0.125)$
                            \item Equality: $2D$
                            \item Range: $BD$
                            \item Insert: Search + $2D$
                            \item Delete: Search + $2D$
                        \end{itemize}
              \end{itemize}
    \end{itemize}
\end{multicols}

\end{document}
